<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Troubleshooting &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.40+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Blogs &amp; Publications" href="blogs_publications.html" />
    <link rel="prev" title="Releases" href="releases.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../">2.1.40+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Troubleshooting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#general-usage">General Usage</a></li>
<li class="toctree-l2"><a class="reference internal" href="#library-dependencies">Library Dependencies</a></li>
<li class="toctree-l2"><a class="reference internal" href="#performance-issue">Performance Issue</a></li>
<li class="toctree-l2"><a class="reference internal" href="#unit-test">Unit Test</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Troubleshooting</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/known_issues.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="troubleshooting">
<h1>Troubleshooting<a class="headerlink" href="#troubleshooting" title="Link to this heading"></a></h1>
<section id="general-usage">
<h2>General Usage<a class="headerlink" href="#general-usage" title="Link to this heading"></a></h2>
<ul>
<li><p><strong>Problem</strong>: FP64 data type is unsupported on current platform.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: FP64 is not natively supported by the <a class="reference external" href="https://www.intel.com/content/www/us/en/products/docs/discrete-gpus/data-center-gpu/flex-series/overview.html">Intel® Data Center GPU Flex Series</a> and <a class="reference external" href="https://www.intel.com/content/www/us/en/products/details/discrete-gpus/arc.html">Intel® Arc™ A-Series Graphics</a> platforms.
If you run any AI workload on that platform and receive this error message, it means a kernel requires FP64 instructions that are not supported and the execution is stopped.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Runtime error <code class="docutils literal notranslate"><span class="pre">invalid</span> <span class="pre">device</span> <span class="pre">pointer</span></code> if <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code></p>
<ul class="simple">
<li><p><strong>Cause</strong>: Intel® Optimization for Horovod* uses utilities provided by Intel® Extension for PyTorch*. The improper import order causes Intel® Extension for PyTorch* to be unloaded before Intel®
Optimization for Horovod* at the end of the execution and triggers this error.</p></li>
<li><p><strong>Solution</strong>: Do <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">intel_extension_for_pytorch</span></code> before <code class="docutils literal notranslate"><span class="pre">import</span> <span class="pre">horovod.torch</span> <span class="pre">as</span> <span class="pre">hvd</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Number of dpcpp devices should be greater than zero.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: If you use Intel® Extension for PyTorch* in a conda environment, you might encounter this error. Conda also ships the libstdc++.so dynamic library file that may conflict with the one shipped
in the OS.</p></li>
<li><p><strong>Solution</strong>: Export the <code class="docutils literal notranslate"><span class="pre">libstdc++.so</span></code> file path in the OS to an environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Symbol undefined caused by <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ImportError:<span class="w"> </span>undefined<span class="w"> </span>symbol:<span class="w"> </span>_ZNK5torch8autograd4Node4nameB5cxx11Ev
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Cause</strong>: DPC++ does not support <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>, Intel® Extension for PyTorch* is always compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. This symbol undefined issue appears when PyTorch* is
compiled with <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=0</span></code>.</p></li>
<li><p><strong>Solution</strong>: Pass <code class="docutils literal notranslate"><span class="pre">export</span> <span class="pre">GLIBCXX_USE_CXX11_ABI=1</span></code> and compile PyTorch* with particular compiler which supports <code class="docutils literal notranslate"><span class="pre">_GLIBCXX_USE_CXX11_ABI=1</span></code>. We recommend using prebuilt wheels
in [download server](https:// developer.intel.com/ipex-whl-stable-xpu) to avoid this issue.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Bad termination after AI model execution finishes when using Intel MPI.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: This is a random issue when the AI model (e.g. RN50 training) execution finishes in an Intel MPI environment. It is not user-friendly as the model execution ends ungracefully. It has been fixed in PyTorch* 2.3 (<a class="reference external" href="https://github.com/pytorch/pytorch/commit/f657b2b1f8f35aa6ee199c4690d38a2b460387ae">#116312</a>).</p></li>
<li><p><strong>Solution</strong>: Add <code class="docutils literal notranslate"><span class="pre">dist.destroy_process_group()</span></code> during the cleanup stage in the model script, as described
in <a class="reference external" href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html">Getting Started with Distributed Data Parallel</a>, before Intel® Extension for PyTorch* supports PyTorch* 2.3.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> when running some AI models on Intel® Arc™ A-Series GPUs.</p>
<ul class="simple">
<li><p><strong>Cause</strong>:  Some of the <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> are actually out-of-memory errors. As Intel® Arc™ A-Series GPUs have less device memory than Intel® Data Center GPU Flex Series 170 and Intel® Data Center GPU
Max  Series, running some AI models on them may trigger out-of-memory errors and cause them to report failure such as <code class="docutils literal notranslate"><span class="pre">-997</span> <span class="pre">runtime</span> <span class="pre">error</span></code> most likely. This is expected. Memory usage optimization is a work in progress to allow Intel® Arc™ A-Series GPUs to support more AI models.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Building from source for Intel® Arc™ A-Series GPUs fails on WSL2 without any error thrown.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Your system probably does not have enough RAM, so Linux kernel’s Out-of-memory killer was invoked. You can verify this by running <code class="docutils literal notranslate"><span class="pre">dmesg</span></code> on bash (WSL2 terminal).</p></li>
<li><p><strong>Solution</strong>: If the OOM killer had indeed killed the build process, then you can try increasing the swap-size of WSL2, and/or decreasing the number of parallel build jobs with the environment
variable <code class="docutils literal notranslate"><span class="pre">MAX_JOBS</span></code> (by default, it’s equal to the number of logical CPU cores. So, setting <code class="docutils literal notranslate"><span class="pre">MAX_JOBS</span></code> to 1 is a very conservative approach that would slow things down a lot).</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Some workloads terminate with an error <code class="docutils literal notranslate"><span class="pre">CL_DEVICE_NOT_FOUND</span></code> after some time on WSL2.</p>
<ul class="simple">
<li><p><strong>Cause</strong>:  This issue is due to the <a class="reference external" href="https://learn.microsoft.com/en-us/windows-hardware/drivers/display/tdr-registry-keys#tdrdelay">TDR feature</a> on Windows.</p></li>
<li><p><strong>Solution</strong>: Try increasing TDRDelay in your Windows Registry to a large value, such as 20 (it is 2 seconds, by default), and reboot.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Random bad termination after AI model convergence test (&gt;24 hours) finishes.</p>
<ul class="simple">
<li><p><strong>Cause</strong>: This is a random issue when some AI model convergence test execution finishes. It is not user-friendly as the model execution ends ungracefully.</p></li>
<li><p><strong>Solution</strong>: Kill the process after the convergence test finished, or use checkpoints to divide the convergence test into several phases and execute separately.</p></li>
</ul>
</li>
<li><p><strong>Problem</strong>: Runtime error <code class="docutils literal notranslate"><span class="pre">munmap_chunk():</span> <span class="pre">invalid</span> <span class="pre">pointer</span></code> when executing some scaling LLM workloads on Intel® Data Center GPU Max Series platform</p>
<ul class="simple">
<li><p><strong>Cause</strong>: Users targeting GPU use, must set the environment variable ‘FI_HMEM=system’ to disable GPU support in underlying libfabric as Intel® MPI Library 2021.13.1 will offload the GPU support instead. This avoids a potential bug in libfabric GPU initialization.</p></li>
<li><p><strong>Solution</strong>: Set the environment variable ‘FI_HMEM=system’ to workaround this issue when encounter.</p></li>
</ul>
</li>
</ul>
</section>
<section id="library-dependencies">
<h2>Library Dependencies<a class="headerlink" href="#library-dependencies" title="Link to this heading"></a></h2>
<ul>
<li><p><strong>Problem</strong>: Cannot find oneMKL library when building Intel® Extension for PyTorch* without oneMKL.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_sycl
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_intel_ilp64
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_core
/usr/bin/ld:<span class="w"> </span>cannot<span class="w"> </span>find<span class="w"> </span>-lmkl_tbb_thread
dpcpp:<span class="w"> </span>error:<span class="w"> </span>linker<span class="w"> </span><span class="nb">command</span><span class="w"> </span>failed<span class="w"> </span>with<span class="w"> </span><span class="nb">exit</span><span class="w"> </span>code<span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">(</span>use<span class="w"> </span>-v<span class="w"> </span>to<span class="w"> </span>see<span class="w"> </span>invocation<span class="o">)</span>
</pre></div>
</div>
<ul>
<li><p><strong>Cause</strong>: When PyTorch* is built with oneMKL library and Intel® Extension for PyTorch* is built without MKL library, this linker issue may occur.</p></li>
<li><p><strong>Solution</strong>: Resolve the issue by setting:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">USE_ONEMKL</span><span class="o">=</span>OFF
<span class="nb">export</span><span class="w"> </span><span class="nv">MKL_DPCPP_ROOT</span><span class="o">=</span><span class="si">${</span><span class="nv">HOME</span><span class="si">}</span>/intel/oneapi/mkl/latest
</pre></div>
</div>
</li>
</ul>
<p>Then clean build Intel® Extension for PyTorch*.</p>
</li>
<li><p><strong>Problem</strong>: Undefined symbol: <code class="docutils literal notranslate"><span class="pre">mkl_lapack_dspevd</span></code>. Intel MKL FATAL ERROR: cannot load <code class="docutils literal notranslate"><span class="pre">libmkl_vml_avx512.so.2</span></code> or `libmkl_vml_def.so.2.</p>
<ul>
<li><p><strong>Cause</strong>: This issue may occur when Intel® Extension for PyTorch* is built with oneMKL library and PyTorch* is not build with any MKL library. The oneMKL kernel may run into CPU backend incorrectly
and trigger this issue.</p></li>
<li><p><strong>Solution</strong>: Resolve the issue by installing the oneMKL library from conda:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>install<span class="w"> </span>mkl
conda<span class="w"> </span>install<span class="w"> </span>mkl-include
</pre></div>
</div>
</li>
</ul>
<p>Then clean build PyTorch*.</p>
</li>
<li><p><strong>Problem</strong>: OSError: <code class="docutils literal notranslate"><span class="pre">libmkl_intel_lp64.so.2</span></code>: cannot open shared object file: No such file or directory.</p>
<ul>
<li><p><strong>Cause</strong>: Wrong MKL library is used when multiple MKL libraries exist in system.</p></li>
<li><p><strong>Solution</strong>: Preload oneMKL by:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">LD_PRELOAD</span><span class="o">=</span><span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_lp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_intel_ilp64.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_gnu_thread.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_core.so.2:<span class="si">${</span><span class="nv">MKL_DPCPP_ROOT</span><span class="si">}</span>/lib/intel64/libmkl_sycl.so.2
</pre></div>
</div>
<p>If you continue seeing similar issues for other shared object files, add the corresponding files under <code class="docutils literal notranslate"><span class="pre">${MKL_DPCPP_ROOT}/lib/intel64/</span></code> by <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code>. Note that the suffix of the libraries may change (e.g. from .1 to .2), if more than one oneMKL library is installed on the system.</p>
</li>
</ul>
</li>
</ul>
</section>
<section id="performance-issue">
<h2>Performance Issue<a class="headerlink" href="#performance-issue" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Problem</strong>: Extended durations for data transfers from the host system to the device (H2D) and from the device back to the host system (D2H).</p>
<ul>
<li><p><strong>Cause</strong>: Absence of certain Dynamic Kernel Module Support (DKMS) packages on Ubuntu 22.04 or earlier versions.</p></li>
<li><p><strong>Solution</strong>: For those running Ubuntu 22.04 or below, it’s crucial to follow all the recommended installation procedures, including those labeled as <a class="reference external" href="https://dgpu-docs.intel.com/driver/client/overview.html#optional-out-of-tree-kernel-mode-driver-install">optional</a>. These steps are likely necessary to install the missing DKMS packages and ensure your system is functioning optimally. The Kernel Mode Driver (KMD) package that addresses this issue has been integrated into the Linux kernel for Ubuntu 23.04 and subsequent releases.</p></li>
</ul>
</li>
</ul>
</section>
<section id="unit-test">
<h2>Unit Test<a class="headerlink" href="#unit-test" title="Link to this heading"></a></h2>
<ul>
<li><p>Unit test failures on Intel® Data Center GPU Flex Series 170</p>
<p>The following unit test fails on Intel® Data Center GPU Flex Series 170 but the same test case passes on Intel® Data Center GPU Max Series. The root cause of the failure is under investigation.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">test_weight_norm.py::TestNNMethod::test_weight_norm_differnt_type</span></code></p></li>
</ul>
</li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="releases.html" class="btn btn-neutral float-left" title="Releases" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="blogs_publications.html" class="btn btn-neutral float-right" title="Blogs &amp; Publications" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f04d85d57e0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a> <a href="/#" data-wap_ref="dns" id="wap_dns"><small>| Your Privacy Choices</small></a> <a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref="nac" id="wap_nac"><small>| Notice at Collection</small></a> </div> <p></p> <div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>. </div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>