<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Transformers Optimization Frontend API &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.40+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Weight-Only Quantization (Prototype)" href="int4_weight_only_quantization.html" />
    <link rel="prev" title="Large Language Models (LLM) Optimizations Overview" href="../llm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../../">2.1.40+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Transformers Optimization Frontend API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode-of-common-usage-scenarios">Pseudocode of Common Usage Scenarios</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fp16">FP16</a></li>
<li class="toctree-l4"><a class="reference internal" href="#smoothquant">SmoothQuant</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#imperative-mode">Imperative mode</a></li>
<li class="toctree-l5"><a class="reference internal" href="#torchscript-mode">TorchScript Mode</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-inference-with-deepspeed">Distributed Inference with DeepSpeed</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#optimized-models">Optimized Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#optimization-methodologies">Optimization Methodologies</a></li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#weight-only-quantization-int4">Weight Only Quantization INT4</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../llm.html">Large Language Models (LLM) Optimizations Overview</a></li>
      <li class="breadcrumb-item active">Transformers Optimization Frontend API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/llm/llm_optimize_transformers.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="transformers-optimization-frontend-api">
<h1>Transformers Optimization Frontend API<a class="headerlink" href="#transformers-optimization-frontend-api" title="Link to this heading"></a></h1>
<p>The new API function, <code class="docutils literal notranslate"><span class="pre">ipex.optimize_transformers</span></code>, is designed to optimize transformer-based models within frontend Python modules, with a particular focus on Large Language Models (LLMs). It provides optimizations for both model-wise and content-generation-wise. You just need to invoke the <code class="docutils literal notranslate"><span class="pre">ipex.optimize_transformers</span></code> function instead of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> function to apply all optimizations transparently.</p>
<p>This API currently works for inference workloads. Support for training is undergoing. Currently, this API supports certain models. Supported model list can be found at <a class="reference external" href="../llm.html#optimized-models">Overview</a>.</p>
<p>API documentation is available at <a class="reference external" href="../api_doc.html#ipex.optimize_transformers">API Docs page</a>.</p>
<section id="pseudocode-of-common-usage-scenarios">
<h2>Pseudocode of Common Usage Scenarios<a class="headerlink" href="#pseudocode-of-common-usage-scenarios" title="Link to this heading"></a></h2>
<p>The following sections show pseudocode snippets to invoke Intel® Extension for PyTorch* APIs to work with LLMs. Complete examples can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.40%2Bxpu/examples/gpu/inference/python/llm">the Example directory</a>.</p>
<section id="fp16">
<h3>FP16<a class="headerlink" href="#fp16" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">transformers</span>


<span class="n">device</span> <span class="o">=</span> <span class="s2">&quot;xpu&quot;</span>
<span class="n">model</span><span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

<span class="n">amp_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span> 
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize_transformers</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">amp_dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># inference with model.generate()</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
<section id="smoothquant">
<h3>SmoothQuant<a class="headerlink" href="#smoothquant" title="Link to this heading"></a></h3>
<p>Supports INT8.</p>
<section id="imperative-mode">
<h4>Imperative mode<a class="headerlink" href="#imperative-mode" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span>

<span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;xpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">modelImpe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantWrapper</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># Define QConfig</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QConfig</span><span class="p">(</span><span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">MinMaxObserver</span> <span class="o">.</span><span class="n">with_args</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_symmetric</span><span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_weight_observer</span><span class="p">)</span>  <span class="c1"># weight could also be perchannel</span>

<span class="n">modelImpe</span><span class="o">.</span><span class="n">qconfig</span> <span class="o">=</span> <span class="n">qconfig</span>

<span class="c1"># Prepare model for inserting observer</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">modelImpe</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calibration to obtain statistics for Observer</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calib_dataset</span><span class="p">:</span>
    <span class="n">modelImpe</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Convert model to create a quantized module</span>
<span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">modelImpe</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Inference</span>
<span class="n">modelImpe</span><span class="p">(</span><span class="n">inference_data</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="torchscript-mode">
<h4>TorchScript Mode<a class="headerlink" href="#torchscript-mode" title="Link to this heading"></a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span>
<span class="kn">from</span> <span class="nn">torch.quantization.quantize_jit</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">convert_jit</span><span class="p">,</span>
    <span class="n">prepare_jit</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Define model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;xpu&quot;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># Generate a ScriptModule</span>
<span class="n">modelJit</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span> <span class="c1"># or torch.jit.script(model)</span>

<span class="c1"># Defin QConfig</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QConfig</span><span class="p">(</span>
    <span class="n">activation</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">observer</span><span class="o">.</span><span class="n">MinMaxObserver</span><span class="o">.</span><span class="n">with_args</span><span class="p">(</span>
        <span class="n">qscheme</span><span class="o">=</span><span class="n">qscheme</span><span class="p">,</span>
        <span class="n">reduce_range</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span>
    <span class="p">),</span>
    <span class="n">weight</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_weight_observer</span>
<span class="p">)</span>

<span class="c1"># Prepare model for inserting observer</span>
<span class="n">modelJit</span> <span class="o">=</span> <span class="n">prepare_jit</span><span class="p">(</span><span class="n">modelJit</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;&#39;</span><span class="p">:</span> <span class="n">qconfig</span><span class="p">},</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Calibration </span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calib_dataset</span><span class="p">:</span>
    <span class="n">modelJit</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="c1"># Convert model to quantized one</span>
<span class="n">modelJit</span> <span class="o">=</span> <span class="n">convert_jit</span><span class="p">(</span><span class="n">modelJit</span><span class="p">)</span>

<span class="c1"># Warmup to fully trigger fusion patterns</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">modelJit</span><span class="p">(</span><span class="n">warmup_data</span><span class="p">)</span> 
<span class="c1"># Inference</span>
<span class="n">modelJit</span><span class="p">(</span><span class="n">inference_data</span><span class="p">)</span>

<span class="c1"># Debug</span>
<span class="nb">print</span><span class="p">(</span><span class="n">modelJit</span><span class="o">.</span><span class="n">graph_for</span><span class="p">(</span><span class="n">inference_dta</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="distributed-inference-with-deepspeed">
<h3>Distributed Inference with DeepSpeed<a class="headerlink" href="#distributed-inference-with-deepspeed" title="Link to this heading"></a></h3>
<p>Distributed inference can be performed with <code class="docutils literal notranslate"><span class="pre">DeepSpeed</span></code>. Based on original Intel® Extension for PyTorch* scripts, the following code changes are required.</p>
<p>Check Distributed Examples in <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.40%2Bxpu/examples/gpu/inference/python/llm">LLM example</a> for complete codes.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../llm.html" class="btn btn-neutral float-left" title="Large Language Models (LLM) Optimizations Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="int4_weight_only_quantization.html" class="btn btn-neutral float-right" title="Weight-Only Quantization (Prototype)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f04d8461cc0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a> <a href="/#" data-wap_ref="dns" id="wap_dns"><small>| Your Privacy Choices</small></a> <a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref="nac" id="wap_nac"><small>| Notice at Collection</small></a> </div> <p></p> <div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>. </div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>