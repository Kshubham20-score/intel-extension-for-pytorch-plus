From 60c90f06aeee12db6784cadb21941bd9ea6f6d78 Mon Sep 17 00:00:00 2001
From: "Yu, Guangye" <106960996+guangyey@users.noreply.github.com>
Date: Tue, 16 Jul 2024 11:29:59 +0800
Subject: [PATCH 39/39] Support XPU on the Accelerator (#274)

* XPUHooksInterface inherits from AcceleratorHooksInterface

ghstack-source-id: bae6143016ae68865a4a298f5af8f35c469fd7c4
Pull Request resolved: https://github.com/pytorch/pytorch/pull/129463

(cherry picked from commit 9a2b8bed3115c68782f4b82cc14fdd7203abc28d)

* Add xpu to getAccelerator

ghstack-source-id: a961cf02eafe0bc6289d703409dd2b7d1fd7cea6
Pull Request resolved: https://github.com/pytorch/pytorch/pull/129205

(cherry picked from commit 2b46c4dd2f5c8758d7736a101d529771038bbe93)
(cherry picked from commit 8218347e8533d90974ca443ef4f212e4c78b07d3)

* Introduce the concept of Accelerators to PyTorch doc

ghstack-source-id: ca60db3005c87faa562fd963c3160f40d385c93f
Pull Request resolved: https://github.com/pytorch/pytorch/pull/129363

(cherry picked from commit 6ee8655905039d990b28cdd4b7d0f13ed186b755)

* Refine the logic of device construction when only device index is given

ghstack-source-id: 2d88b54f74342a330c1bee53d2dba94428582bbf
Pull Request resolved: https://github.com/pytorch/pytorch/pull/129119

(cherry picked from commit 8760230f2627b706b73a86b71406288b1ae5b9ad)
---
 aten/src/ATen/Context.h                  |   2 +
 aten/src/ATen/DeviceAccelerator.cpp      |  50 +++--
 aten/src/ATen/DeviceAccelerator.h        |   4 +-
 aten/src/ATen/detail/XPUHooksInterface.h |  32 ++-
 aten/src/ATen/xpu/detail/XPUHooks.cpp    |   5 +
 aten/src/ATen/xpu/detail/XPUHooks.h      |   1 +
 docs/source/tensor_attributes.rst        |   3 +-
 docs/source/torch.rst                    |  21 ++
 torch/_torch_docs.py                     | 259 +++++++++++++++++++++++
 torch/csrc/utils/python_arg_parser.h     |   9 +-
 10 files changed, 336 insertions(+), 50 deletions(-)

diff --git a/aten/src/ATen/Context.h b/aten/src/ATen/Context.h
index 17910136ab0..dbd8b565a8e 100644
--- a/aten/src/ATen/Context.h
+++ b/aten/src/ATen/Context.h
@@ -64,6 +64,8 @@ class TORCH_API Context {
         : at::getAccelerator(true).value();
     if (device_type == at::kCUDA) {
       return at::detail::getCUDAHooks();
+    } else if (device_type == at::kXPU) {
+      return at::detail::getXPUHooks();
     } else if (device_type == at::kMPS) {
       return at::detail::getMPSHooks();
     } else if (device_type == at::kPrivateUse1) {
diff --git a/aten/src/ATen/DeviceAccelerator.cpp b/aten/src/ATen/DeviceAccelerator.cpp
index 05327cc219e..5b093cc9cbc 100644
--- a/aten/src/ATen/DeviceAccelerator.cpp
+++ b/aten/src/ATen/DeviceAccelerator.cpp
@@ -1,31 +1,37 @@
-#include <ATen/DeviceAccelerator.h>
 #include <ATen/Context.h>
-
+#include <ATen/DeviceAccelerator.h>
 namespace at {
 
 C10_API std::optional<DeviceType> getAccelerator(bool checked) {
-#define CHECK_NO_CUDA \
-  TORCH_CHECK(!at::hasCUDA(), "Cannot have both CUDA and PrivateUse1");
+#define DETECT_AND_ASSIGN_ACCELERATOR(device_name) \
+  if (at::has##device_name()) {                    \
+    device_type = k##device_name;                  \
+    TORCH_CHECK(                                   \
+        !is_accelerator_detected,                  \
+        "Cannot have ",                            \
+        device_type.value(),                       \
+        " with other accelerators.");              \
+    is_accelerator_detected = true;                \
+  }
 
-#define CHECK_NO_PU1 \
-  TORCH_CHECK(!is_privateuse1_backend_registered(), "Cannot have both CUDA and PrivateUse1");
+  if (is_privateuse1_backend_registered()) {
+    // We explicitly allow PrivateUse1 and another device at the same time as we
+    // use this for testing. Whenever a PrivateUse1 device is registered, use it
+    // first.
+    return kPrivateUse1;
+  }
+  std::optional<DeviceType> device_type = std::nullopt;
+  bool is_accelerator_detected = false;
+  DETECT_AND_ASSIGN_ACCELERATOR(CUDA)
+  DETECT_AND_ASSIGN_ACCELERATOR(MTIA)
+  DETECT_AND_ASSIGN_ACCELERATOR(XPU)
+  if (checked) {
+    TORCH_CHECK(
+        device_type, "Cannot access accelerator device when none is available.")
+  }
+  return device_type;
 
-    if (is_privateuse1_backend_registered()) {
-        // We explicitly allow PrivateUse1 and another device at the same time
-        // as we use this for testing.
-        // Whenever a PrivateUse1 device is registered, use it first.
-        return kPrivateUse1;
-    } else if (at::hasCUDA()) {
-        CHECK_NO_PU1
-        return kCUDA;
-    } else {
-        TORCH_CHECK(!checked, "Cannot access accelerator device when none is available.")
-        return std::nullopt;
-    }
-
-#undef CHECK_NO_CUDA
-#undef CHECK_NO_PU1
+#undef DETECT_AND_ASSIGN_ACCELERATOR
 }
 
-
 } // namespace at
diff --git a/aten/src/ATen/DeviceAccelerator.h b/aten/src/ATen/DeviceAccelerator.h
index c3e800c7e07..3eedb9945ef 100644
--- a/aten/src/ATen/DeviceAccelerator.h
+++ b/aten/src/ATen/DeviceAccelerator.h
@@ -13,9 +13,9 @@
 // - It provides a set of common APIs as defined by AcceleratorHooksInterface
 //
 // As of today, accelerator devices are (in no particular order):
-// CUDA, MTIA, PrivateUse1
+// CUDA, MTIA, XPU, PrivateUse1
 // We want to add once all the proper APIs are supported and tested:
-// HIP, MPS, XPU
+// HIP, MPS
 
 namespace at {
 
diff --git a/aten/src/ATen/detail/XPUHooksInterface.h b/aten/src/ATen/detail/XPUHooksInterface.h
index 8e5e0d8243a..82cc0e9ef95 100644
--- a/aten/src/ATen/detail/XPUHooksInterface.h
+++ b/aten/src/ATen/detail/XPUHooksInterface.h
@@ -2,30 +2,21 @@
 
 #include <c10/core/Device.h>
 #include <c10/util/Exception.h>
-#include <ATen/core/Generator.h>
 #include <c10/util/Registry.h>
 
-#include <cstddef>
-#include <functional>
-#include <memory>
+#include <ATen/core/Generator.h>
+#include <ATen/detail/AcceleratorHooksInterface.h>
 
-namespace at {
 
-constexpr const char* XPU_HELP =
-    "The XPU backend requires Intel Extension for Pytorch;"
-    "this error has occurred because you are trying "
-    "to use some XPU's functionality, but the Intel Extension for Pytorch has not been "
-    "loaded for some reason. The Intel Extension for Pytorch MUST "
-    "be loaded, EVEN IF you don't directly use any symbols from that!";
+namespace at {
 
-struct TORCH_API XPUHooksInterface {
-  virtual ~XPUHooksInterface() {}
+struct TORCH_API XPUHooksInterface : AcceleratorHooksInterface{
+  ~XPUHooksInterface() override = default;
 
   virtual void initXPU() const {
     TORCH_CHECK(
         false,
-        "Cannot initialize XPU without Intel Extension for Pytorch.",
-        XPU_HELP);
+        "Cannot initialize XPU without ATen_xpu library.");
   }
 
   virtual bool hasXPU() const {
@@ -35,8 +26,7 @@ struct TORCH_API XPUHooksInterface {
   virtual std::string showConfig() const {
     TORCH_CHECK(
         false,
-        "Cannot query detailed XPU version without Intel Extension for Pytorch. ",
-        XPU_HELP);
+        "Cannot query detailed XPU version without ATen_xpu library.");
   }
 
   virtual int32_t getGlobalIdxFromDevice(const Device& device) const {
@@ -44,11 +34,11 @@ struct TORCH_API XPUHooksInterface {
   }
 
   virtual Generator getXPUGenerator(C10_UNUSED DeviceIndex device_index = -1) const {
-    TORCH_CHECK(false, "Cannot get XPU generator without Intel Extension for Pytorch. ", XPU_HELP);
+    TORCH_CHECK(false, "Cannot get XPU generator without ATen_xpu library.");
   }
 
   virtual const Generator& getDefaultXPUGenerator(C10_UNUSED DeviceIndex device_index = -1) const {
-    TORCH_CHECK(false, "Cannot get default XPU generator without Intel Extension for Pytorch. ", XPU_HELP);
+    TORCH_CHECK(false, "Cannot get default XPU generator without ATen_xpu library.");
   }
 
   virtual DeviceIndex getNumGPUs() const {
@@ -74,6 +64,10 @@ struct TORCH_API XPUHooksInterface {
   virtual bool isPinnedPtr(const void* /*data*/) const {
     return false;
   }
+
+  virtual bool hasPrimaryContext(DeviceIndex /*device_index*/) const override{
+    TORCH_CHECK(false, "Cannot query primary context without ATen_xpu library.");
+  }
 };
 
 struct TORCH_API XPUHooksArgs {};
diff --git a/aten/src/ATen/xpu/detail/XPUHooks.cpp b/aten/src/ATen/xpu/detail/XPUHooks.cpp
index 61bc19faa95..589e792ef47 100644
--- a/aten/src/ATen/xpu/detail/XPUHooks.cpp
+++ b/aten/src/ATen/xpu/detail/XPUHooks.cpp
@@ -80,6 +80,11 @@ bool XPUHooks::isPinnedPtr(const void* data) const {
       sycl::get_pointer_type(data, c10::xpu::get_device_context());
 }
 
+bool XPUHooks::hasPrimaryContext(DeviceIndex device_index) const {
+  // The default context is utilized for each device. So it always returns true.
+  return true;
+}
+
 REGISTER_XPU_HOOKS(XPUHooks);
 
 } // namespace at::xpu::detail
diff --git a/aten/src/ATen/xpu/detail/XPUHooks.h b/aten/src/ATen/xpu/detail/XPUHooks.h
index 30279583ff0..b417f508e49 100644
--- a/aten/src/ATen/xpu/detail/XPUHooks.h
+++ b/aten/src/ATen/xpu/detail/XPUHooks.h
@@ -20,6 +20,7 @@ struct XPUHooks : public at::XPUHooksInterface {
   void deviceSynchronize(DeviceIndex device_index) const override;
   Allocator* getPinnedMemoryAllocator() const override;
   bool isPinnedPtr(const void* data) const override;
+  bool hasPrimaryContext(DeviceIndex device_index) const override;
 };
 
 } // namespace at::xpu::detail
diff --git a/docs/source/tensor_attributes.rst b/docs/source/tensor_attributes.rst
index 9b2b8716fce..78274d6572a 100644
--- a/docs/source/tensor_attributes.rst
+++ b/docs/source/tensor_attributes.rst
@@ -213,7 +213,8 @@ non-None device argument.  To globally change the default device, see also
 
 .. note::
    For legacy reasons, a device can be constructed via a single device ordinal, which is treated
-   as a cuda device.  This matches :meth:`Tensor.get_device`, which returns an ordinal for cuda
+   as the current :ref:`accelerator<accelerators>` type.
+   This matches :meth:`Tensor.get_device`, which returns an ordinal for device
    tensors and is not supported for cpu tensors.
 
    >>> torch.device(1)
diff --git a/docs/source/torch.rst b/docs/source/torch.rst
index 71d467a9e35..d636a6b6018 100644
--- a/docs/source/torch.rst
+++ b/docs/source/torch.rst
@@ -140,6 +140,27 @@ Indexing, Slicing, Joining, Mutating Ops
     vstack
     where
 
+.. _accelerators:
+
+Accelerators
+----------------------------------
+Within the PyTorch repo, we define an "Accelerator" as a :class:`torch.device` that is being used
+alongside a CPU to speed up computation. These device use an asynchronous execution scheme,
+using :class:`torch.Stream` and :class:`torch.Event` as their main way to perform synchronization.
+We also assume that only one such accelerator can be available at once on a given host. This allows
+us to use the current accelerator as the default device for relevant concepts such as pinned memory,
+Stream device_type, etc.
+
+As of today, accelerator devices are (in no particular order) :doc:`"CUDA" <cuda>`, :doc:`"MTIA" <mtia>`,
+:doc:`"XPU" <xpu>`, and PrivateUse1 (many device not in the PyTorch repo itself).
+
+.. autosummary::
+    :toctree: generated
+    :nosignatures:
+
+    Stream
+    Event
+
 .. _generators:
 
 Generators
diff --git a/torch/_torch_docs.py b/torch/_torch_docs.py
index eab7672f028..5acce1835aa 100644
--- a/torch/_torch_docs.py
+++ b/torch/_torch_docs.py
@@ -13661,6 +13661,265 @@ Example::
 )
 
 
+add_docstr(
+    torch.Stream,
+    r"""
+Stream(device, *, priority) -> Stream
+
+An in-order queue of executing the respective tasks asynchronously in first in first out (FIFO) order.
+It can control or synchronize the execution of other Stream or block the current host thread to ensure
+the correct task sequencing.
+
+See in-depth description of the CUDA behavior at :ref:`cuda-semantics` for details
+on the exact semantic that applies to all devices.
+
+Arguments:
+    device (:class:`torch.device`, optional): the desired device for the Stream.
+        If not given, the current :ref:`accelerator<accelerators>` type will be used.
+    priority (int, optional): priority of the stream, should be 0 or negative, where negative
+        numbers indicate higher priority. By default, streams have priority 0.
+
+Returns:
+    Stream: An torch.Stream object.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+""",
+)
+
+
+add_docstr(
+    torch.Stream.query,
+    r"""
+Stream.query() -> bool
+
+Check if all the work submitted has been completed.
+
+Returns:
+    bool: A boolean indicating if all kernels in this stream are completed.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> s_cuda.query()
+    True
+""",
+)
+
+
+add_docstr(
+    torch.Stream.record_event,
+    r"""
+Stream.record_event(event) -> Event
+
+Record an event. En-queuing it into the Stream to allow further synchronization from the current point in the FIFO queue.
+
+Arguments:
+    event (:class:`torch.Event`, optional): event to record. If not given, a new one will be allocated.
+
+Returns:
+    Event: Recorded event.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> e_cuda = s_cuda.record_event()
+""",
+)
+
+
+add_docstr(
+    torch.Stream.synchronize,
+    r"""
+Stream.synchronize() -> None
+
+Wait for all the kernels in this stream to complete.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> s_cuda.synchronize()
+""",
+)
+
+
+add_docstr(
+    torch.Stream.wait_event,
+    r"""
+Stream.wait_event(event) -> None
+
+Make all future work submitted to the stream wait for an event.
+
+Arguments:
+    event (:class:`torch.Event`): an event to wait for.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s1_cuda = torch.Stream(device='cuda')
+    >>> s2_cuda = torch.Stream(device='cuda')
+    >>> e_cuda = s1_cuda.record_event()
+    >>> s2_cuda.wait_event(e_cuda)
+""",
+)
+
+
+add_docstr(
+    torch.Stream.wait_stream,
+    r"""
+Stream.wait_stream(stream) -> None
+
+Synchronize with another stream. All future work submitted to this stream will wait until all kernels
+already submitted to the given stream are completed.
+
+Arguments:
+    stream (:class:`torch.Stream`): a stream to synchronize.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s1_cuda = torch.Stream(device='cuda')
+    >>> s2_cuda = torch.Stream(device='cuda')
+    >>> s2_cuda.wait_stream(s1_cuda)
+""",
+)
+
+
+add_docstr(
+    torch.Event,
+    r"""
+Event(device, *, enable_timing) -> Event
+
+Query and record Stream status to identify or control dependencies across Stream and measure timing.
+
+Arguments:
+    device (:class:`torch.device`, optional): the desired device for the Event.
+        If not given, the current :ref:`accelerator<accelerators>` type will be used.
+    enable_timing (bool, optional): indicates if the event should measure time (default: ``False``).
+
+Returns:
+    Event: An torch.Event object.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> e_cuda = torch.Event(device='cuda')
+""",
+)
+
+
+add_docstr(
+    torch.Event.elapsed_time,
+    r"""
+Event.elapsed_time(end_event) -> float
+
+Returns the elapsed time in milliseconds between when this event and the :attr:`end_event` are
+each recorded via :func:`torch.Stream.record_event`.
+
+Arguments:
+    end_event (:class:`torch.Event`): The ending event has been recorded.
+
+Returns:
+    float: Time between starting and ending event in milliseconds.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> e1_cuda = s_cuda.record_event()
+    >>> e2_cuda = s_cuda.record_event()
+    >>> ms = e1_cuda.elapsed_time(e2_cuda)
+""",
+)
+
+
+add_docstr(
+    torch.Event.query,
+    r"""
+Event.query() -> bool
+
+Check if the stream where this event was recorded already moved past the point where the event was recorded.
+Always returns ``True`` if the Event was not recorded.
+
+Returns:
+    bool: A boolean indicating if all work currently captured by event has completed.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> e_cuda = s_cuda.record_event()
+    >>> e_cuda.query()
+    True
+""",
+)
+
+
+add_docstr(
+    torch.Event.record,
+    r"""
+Event.record(stream) -> None
+
+Record the event in a given stream. The stream's device must match the event's device.
+This function is equivalent to ``stream.record_event(self)``.
+
+Arguments:
+    stream (:class:`torch.Stream`, optional): A stream to be recorded.
+    If not given, the current stream will be used.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> e_cuda = torch.Event(device='cuda')
+    >>> e_cuda.record()
+""",
+)
+
+
+add_docstr(
+    torch.Event.synchronize,
+    r"""
+Event.synchronize() -> None
+
+Wait for the event to complete. This prevents the CPU thread from proceeding until the event completes.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s_cuda = torch.Stream(device='cuda')
+    >>> e_cuda = s_cuda.record_event()
+    >>> e_cuda.synchronize()
+""",
+)
+
+
+add_docstr(
+    torch.Event.wait,
+    r"""
+Event.wait(stream) -> None
+
+Make all future work submitted to the given stream wait for this event.
+
+Arguments:
+    stream (:class:`torch.Stream`, optional): A stream to synchronize.
+    If not given, the current stream will be used.
+
+Example::
+
+    >>> # xdoctest: +REQUIRES(env:TORCH_DOCTEST_CUDA)
+    >>> s1_cuda = torch.Stream(device='cuda')
+    >>> s2_cuda = torch.Stream(device='cuda')
+    >>> e_cuda = s1_cuda.record()
+    >>> e_cuda.wait(s2)
+""",
+)
+
+
 add_docstr(
     torch.Generator,
     r"""
diff --git a/torch/csrc/utils/python_arg_parser.h b/torch/csrc/utils/python_arg_parser.h
index cec99a84330..a03a806023e 100644
--- a/torch/csrc/utils/python_arg_parser.h
+++ b/torch/csrc/utils/python_arg_parser.h
@@ -66,6 +66,7 @@
 #include <torch/csrc/utils/python_symnode.h>
 #include <torch/csrc/utils/six.h>
 
+#include <ATen/DeviceAccelerator.h>
 #include <ATen/PythonTorchFunctionTLS.h>
 #include <ATen/core/Tensor.h>
 #include <c10/util/Exception.h>
@@ -808,13 +809,9 @@ inline at::Device toDevice(PyObject* obj) {
   if (THPUtils_checkLong(obj)) {
     const auto device_index = THPUtils_unpackLong(obj);
     TORCH_CHECK(device_index >= 0, "Device index must not be negative");
-    if (c10::is_privateuse1_backend_registered()) {
-      return at::Device(
-          c10::DeviceType::PrivateUse1,
-          static_cast<c10::DeviceIndex>(device_index));
-    }
     return at::Device(
-        c10::DeviceType::CUDA, static_cast<c10::DeviceIndex>(device_index));
+        at::getAccelerator(true).value(),
+        static_cast<c10::DeviceIndex>(device_index));
   }
   const std::string& device_str = THPUtils_unpackString(obj);
   return at::Device(device_str);
-- 
2.34.1

