From 44d1ed87a2675d0764e32b8839936371da8ea3a8 Mon Sep 17 00:00:00 2001
From: xingyuan li <108672484+hoshibara@users.noreply.github.com>
Date: Fri, 8 Dec 2023 14:48:45 +0800
Subject: [PATCH 21/31] Add sfdp pattern for huggingface Bert fp16 inference
 model (#195)

* Add sfdp pattern for huggingface Bert fp16 inference model (to half version & xpu optimized version)
* Repalce dropout with clone
---
 test/inductor/test_fused_attention.py       | 65 +++++++++++++++++++++
 torch/_inductor/fx_passes/fuse_attention.py | 64 ++++++++++++++++++++
 2 files changed, 129 insertions(+)

diff --git a/test/inductor/test_fused_attention.py b/test/inductor/test_fused_attention.py
index df018354827..0669d004733 100644
--- a/test/inductor/test_fused_attention.py
+++ b/test/inductor/test_fused_attention.py
@@ -495,6 +495,65 @@ class TestSDPAPatternRewriterTemplate(TestCase):
 
         self._check_common(dot_prod_attention)
 
+    @skipIfRocm
+    def _test_sdpa_rewriter_16(self):
+        def dot_prod_attention(
+            query: torch.Tensor, key: torch.Tensor, value: torch.Tensor
+        ) -> torch.Tensor:
+            """Input tensors assumed to have shape (batch_size, seq_len, n_head, embed_dim)"""
+            attn_mask = torch.ones((query.size(0), 1, query.size(1), key.size(1)), 
+                                   dtype=query.dtype, device=query.device
+                                   ).tril(diagonal=0)
+            attn_mask = attn_mask.masked_fill(
+                torch.logical_not(attn_mask), -float("inf")
+            )
+            
+            q = query.permute(0, 2, 1, 3)
+            k = key.permute(0, 2, 1, 3)
+            v = value.permute(0, 2, 1, 3)
+            attention_scores = torch.matmul(q, k.transpose(-1, -2))
+            attention_scores = attention_scores / math.sqrt(q.size(-1))
+            attention_scores = attention_scores + attn_mask
+            attention_probs = torch.nn.functional.softmax(attention_scores, -1)
+            attention_probs = torch.nn.functional.dropout(attention_probs, 0.1, training=False)
+            return torch.matmul(attention_probs, v)
+        
+        args = (
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+        )
+        self._check_common(dot_prod_attention, args)
+
+    @skipIfRocm
+    def _test_sdpa_rewriter_17(self):
+        def dot_prod_attention(
+            query: torch.Tensor, key: torch.Tensor, value: torch.Tensor
+        ) -> torch.Tensor:
+            attn_mask = torch.ones((query.size(0), 1, query.size(1), key.size(1)), 
+                                   dtype=query.dtype, device=query.device
+                                   ).tril(diagonal=0)
+            attn_mask = attn_mask.masked_fill(
+                torch.logical_not(attn_mask), -float("inf")                
+            )
+            
+            """Input tensors assumed to have shape (batch_size, seq_len, n_head, embed_dim)"""
+            q = query.permute(0, 2, 1, 3)
+            k = key.permute(0, 2, 1, 3)
+            v = value.permute(0, 2, 1, 3)
+            attention_scores = torch.matmul(q, k.transpose(-1, -2))
+            attention_scores = attention_scores / math.sqrt(q.size(-1))
+            attention_scores = attention_scores + attn_mask
+            attention_probs = torch.nn.functional.softmax(attention_scores, -1)
+            return torch.matmul(attention_probs, v)
+
+        args = (
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+            torch.randn((2, 8, 4, 16), device=self.device, dtype=torch.half),
+        )
+        self._check_common(dot_prod_attention, args)
+
 
 if HAS_CUDA and PLATFORM_SUPPORTS_FUSED_SDPA:
 
@@ -554,6 +613,12 @@ if HAS_CUDA and PLATFORM_SUPPORTS_FUSED_SDPA:
         test_sdpa_rewriter_15_cuda = (
             TestSDPAPatternRewriterTemplate._test_sdpa_rewriter_15
         )
+        test_sdpa_rewriter_16_cuda = (
+            TestSDPAPatternRewriterTemplate._test_sdpa_rewriter_16
+        )
+        test_sdpa_rewriter_17_cuda = (
+            TestSDPAPatternRewriterTemplate._test_sdpa_rewriter_17
+        )
 
 
 if HAS_CPU:
diff --git a/torch/_inductor/fx_passes/fuse_attention.py b/torch/_inductor/fx_passes/fuse_attention.py
index 333eafa1d5a..59095972d5d 100644
--- a/torch/_inductor/fx_passes/fuse_attention.py
+++ b/torch/_inductor/fx_passes/fuse_attention.py
@@ -378,6 +378,55 @@ def _sfdp_replacement_15(query, key, value, inv_scale):
     )
 
 
+def _sfdp_pattern_16(query, key, value, attn_mask):
+    # For huggingface bert
+    query = query.permute(0, 2, 1, 3)
+    key = key.permute(0, 2, 1, 3)
+    value = value.permute(0, 2, 1, 3)
+    attention_scores = torch.matmul(query, key.transpose(-1, -2))
+    attention_scores = attention_scores / math.sqrt(query.size(-1))
+    attention_scores = attention_scores + attn_mask
+    attention_probs = torch.nn.functional.softmax(attention_scores, -1)
+    # dropout would create a clone() if eval() or p = 0
+    attention_probs = attention_probs.clone()
+    return torch.matmul(attention_probs, value)
+
+
+def _sfdp_replacement_16(query, key, value, attn_mask):
+    counters["inductor"]["fuse_attention"] += 1
+    return aten.scaled_dot_product_attention(
+        query.transpose(1, 2),
+        key.transpose(1, 2),
+        value.transpose(1, 2),
+        attn_mask=attn_mask,
+        dropout_p=0.0,
+        is_causal=False,
+    )
+
+
+def _sfdp_pattern_17(query, key, value, attn_mask):
+    # No dropout version of pattern 16
+    query = query.permute(0, 2, 1, 3)
+    key = key.permute(0, 2, 1, 3)
+    value = value.permute(0, 2, 1, 3)
+    attention_scores = torch.matmul(query, key.transpose(-1, -2))
+    attention_scores = attention_scores / math.sqrt(query.size(-1))
+    attention_scores = attention_scores + attn_mask
+    attention_probs = torch.nn.functional.softmax(attention_scores, -1)
+    return torch.matmul(attention_probs, value)
+
+
+def _sfdp_replacement_17(query, key, value, attn_mask):
+    counters["inductor"]["fuse_attention"] += 1
+    return aten.scaled_dot_product_attention(
+        query.transpose(1, 2),
+        key.transpose(1, 2),
+        value.transpose(1, 2),
+        attn_mask=attn_mask,
+        dropout_p=0.0,
+        is_causal=False,
+    )
+
 def _sfdp_params_check(match):
     assert all(k in match.kwargs for k in ("query", "key", "value"))
     query = match.kwargs["query"].meta["val"]
@@ -435,6 +484,7 @@ def _sfdp_init():
         torch.empty, (2, 8, 4, 16), device=device, requires_grad=True, dtype=torch.half
     )
     b = functools.partial(torch.empty, (1, 1, 8, 8), device=device)
+    bp = functools.partial(torch.empty, (2, 1, 1, 8), device=device, dtype=torch.half)
     c = functools.partial(torch.tensor, 2.0, device=device)
     # workaround https://github.com/pytorch/pytorch/issues/97894
     # 0.113377 is a "magic" value that lets us recover the lost input arg relationship
@@ -546,6 +596,20 @@ def _sfdp_init():
             {},
             _sfdp_scale_factor_check(aten.div.Tensor),
         ),
+        (
+            _sfdp_pattern_16,
+            _sfdp_replacement_16,
+            [gp(), gp(), gp(), bp()],
+            {},
+            _sfdp_params_check,
+        ),
+        (
+            _sfdp_pattern_17,
+            _sfdp_replacement_17,
+            [gp(), gp(), gp(), bp()],
+            {},
+            _sfdp_params_check,
+        ),
     ]:
         args = [*args, *workaround.values()]
         register_replacement(
-- 
2.34.1

