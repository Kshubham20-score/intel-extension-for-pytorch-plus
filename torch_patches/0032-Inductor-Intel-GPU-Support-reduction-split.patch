From aa62f43cd1864c90c389cfcb0ebc34770dbb5700 Mon Sep 17 00:00:00 2001
From: "Xiao, Wang" <109140002+xiaowangintel@users.noreply.github.com>
Date: Fri, 21 Jun 2024 15:37:45 +0800
Subject: [PATCH 32/39] [Inductor][Intel GPU] Support reduction split.

cherry-picked from https://github.com/pytorch/pytorch/pull/129120

---------
Co-authored-by: xiaowangintel <wang.xiao@intel.com>
---
 torch/_inductor/ir.py | 12 ++++++++----
 1 file changed, 8 insertions(+), 4 deletions(-)

diff --git a/torch/_inductor/ir.py b/torch/_inductor/ir.py
index 3b03558a49d..8452e58dee6 100644
--- a/torch/_inductor/ir.py
+++ b/torch/_inductor/ir.py
@@ -680,7 +680,7 @@ class Reduction(Loops):
         numel_hint = V.graph.sizevars.symbolic_hint(sympy_product(ranges))
 
         should_split = (
-            get_device_type(device) == "cuda"
+            is_gpu(get_device_type(device))
             and reduction_type
             not in {
                 "argmax",
@@ -695,9 +695,13 @@ class Reduction(Loops):
             return ReductionHint.DEFAULT, 1
 
         device_interface = get_interface_for_device(get_device_type(device))
-        num_sm = device_interface.Worker.get_device_properties(
-            device
-        ).multi_processor_count
+        device_properties = device_interface.Worker.get_device_properties(device)
+        if get_device_type(device) == "xpu":
+            num_sm = device_properties.gpu_subslice_count
+        else:
+            # default is cuda behavior
+            num_sm = device_properties.multi_processor_count
+
         min_elements_per_thread = 32
         max_elements_per_thread = 512
         threads_per_sm = 2048
-- 
2.34.1

