
/*
Fused Multi-Head Attention Forward

This is an implementation of the Flash Attention algorithm
(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf)
*/

#include "SDP/fmha_forward.hpp"
#include "SDP/fmha_forward_kernel.hpp"
#include "SDP/fmha_forward_v2.hpp"

// clang-format off
// macros to be filled in CMake 
#define IMPL_T ${IMPL_T}
#define IMPL_ARCH_TAG gpu_arch::${IMPL_ARCH_TAG}
#define GPU_ARCH_${IMPL_ARCH_TAG}
#cmakedefine01 IMPL_KUSEALIBI
#cmakedefine01 IMPL_KUSEBIAS
#cmakedefine01 IMPL_KISCAUSAL
#cmakedefine01 IMPL_KSEQLAST
#cmakedefine01 IMPL_KISTRAINING
#cmakedefine01 IMPL_KISDROPOUT
#cmakedefine01 IMPL_KISVARLEN
// clang-format on

namespace gpu::xetla {

namespace fmha {

template <typename fmha_policy>
struct is_fmha_v2 : std::false_type {};

template <int head_dim>
struct is_fmha_v2<std::integral_constant<int, head_dim>> : std::true_type {};

// The launcher of fmha forward kernel
template <
    typename fmha_policy,
    typename T,
    gpu_arch arch_tag,
    bool kUseAlibi,
    bool kUseBias,
    bool kIsCausal,
    bool kSeqLast,
    bool kIsTraining,
    bool kIsDropout,
    bool kVarlen>
cgfs_t xetla_fmha_forward_kernel(const dispatch_fmha_forward_args_t<T>& args) {
#ifdef SDP_DBG
  printf(
      "B, N, Nkv, F, T, H: %u, %u, %u, %u, %u, %u, UseAlibi: %d, UseBias: %d, IsCausal: %d, IsTraining: %d,"
      "IsDropout: %d, IsVarlen: %d, alibi @ 0x%llx, uAT %d, uMT %d, strideB %d, strideN %d, strideF %d, dropout_prob %f, kSeqLast %d\n",
      args.num_batches,
      args.num_heads,
      args.num_kv_heads,
      args.num_queries,
      args.num_keys,
      args.head_size,
      kUseAlibi,
      kUseBias,
      kIsCausal,
      kIsTraining,
      kIsDropout,
      kIsVarlen,
      (unsigned long long)args.alibi,
      args.alibi_padded_block_size,
      args.attn_mask_padded_block_size,
      args.bias_strideB,
      args.bias_strideN,
      args.bias_strideF,
      args.dropout_prob,
      kSeqLast);
#endif
  constexpr bool kUseV2 = is_fmha_v2<fmha_policy>::value;
  // fmha forward kernel

  if constexpr (!kUseV2) {
    using fmha_forward_op_t = fmha_forward_t<
        fmha_policy,
        T,
        arch_tag,
        kUseAlibi,
        kUseBias,
        kIsCausal,
        kSeqLast,
        kIsTraining,
        kIsDropout,
        kVarlen>;

    sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(
        args.num_batches * args.num_heads, args.num_queries);

    FmhaForwardKernelFunctor<fmha_forward_op_t, T, kUseV2> kfn(args);
    return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};
  } else {
    using fmha_forward_op_t =
        fmha_forward_v2_t<T, arch_tag, 64, fmha_policy::value, kUseBias>;

    sycl::nd_range<3> NdRange =
        fmha_forward_op_t::get_nd_range(args.num_batches, args.num_heads);

    FmhaForwardKernelFunctor<fmha_forward_op_t, T, kUseV2> kfn(args);
    return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};
  }
}

#define INSTANTIATE_POLICY(policy)           \
  template cgfs_t xetla_fmha_forward_kernel< \
      policy,                                \
      IMPL_T,                                \
      IMPL_ARCH_TAG,                         \
      IMPL_KUSEALIBI,                        \
      IMPL_KUSEBIAS,                         \
      IMPL_KISCAUSAL,                        \
      IMPL_KSEQLAST,                         \
      IMPL_KISTRAINING,                      \
      IMPL_KISDROPOUT,                       \
      IMPL_KISVARLEN>(const dispatch_fmha_forward_args_t<IMPL_T>& args)

#if (IMPL_KISTRAINING && IMPL_KISDROPOUT && defined(GPU_ARCH_XeHpc))
INSTANTIATE_POLICY(fmha_policy_128x128x64);
INSTANTIATE_POLICY(fmha_policy_128x128x128);
INSTANTIATE_POLICY(fmha_policy_128x128x256);
INSTANTIATE_POLICY(fmha_policy_64x128x512);
#endif

INSTANTIATE_POLICY(fmha_policy_8x128x64);
INSTANTIATE_POLICY(fmha_policy_64x128x64);

#if (!IMPL_KISDROPOUT && defined(GPU_ARCH_XeLpg))
#define COMMA ,
INSTANTIATE_POLICY(fmha_policy_1x256x128);
INSTANTIATE_POLICY(fmha_policy_1x512x128);
INSTANTIATE_POLICY(std::integral_constant<int COMMA 64>);
INSTANTIATE_POLICY(std::integral_constant<int COMMA 128>);
#endif

INSTANTIATE_POLICY(fmha_policy_8x256x128);
INSTANTIATE_POLICY(fmha_policy_8x512x128);

#if defined(GPU_ARCH_XeLpg)
INSTANTIATE_POLICY(fmha_policy_32x128x128);
#else
INSTANTIATE_POLICY(fmha_policy_64x128x128);
#endif

#if defined(GPU_ARCH_XeLpg)
#else
INSTANTIATE_POLICY(fmha_policy_8x256x256);
INSTANTIATE_POLICY(fmha_policy_64x128x256);
#if defined(GPU_ARCH_XeHpc)
INSTANTIATE_POLICY(fmha_policy_64x256x256);
#if !(IMPL_KISTRAINING && IMPL_KISDROPOUT)
INSTANTIATE_POLICY(fmha_policy_64x128x512);
#endif
#endif
#endif
} // namespace fmha
} // namespace gpu::xetla
