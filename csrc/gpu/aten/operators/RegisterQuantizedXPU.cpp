// required for old g++ to compile PRId64 macros, see
// https://github.com/pytorch/pytorch/issues/3571
// for context
#ifndef __STDC_FORMAT_MACROS
#define __STDC_FORMAT_MACROS
#endif

// an external backend might generate file within its code tree
// and check all the source files within the tree with clang-format.
// so, disable it since the backend might have a different config.
// clang-format off

// NOTE: This condition is true for all PyTorch internal libraries, it
//       just excludes external projects such as torch_xla which
//       re-use some of the PyTorch codegen machinery.
#if defined(CAFFE2_BUILD_MAIN_LIB)        || \
    defined(TORCH_CUDA_BUILD_MAIN_LIB)    || \
    defined(TORCH_HIP_BUILD_MAIN_LIB)     || \
    defined(TORCH_CUDA_CU_BUILD_MAIN_LIB) || \
    defined(TORCH_CUDA_CPP_BUILD_MAIN_LIB)
#define TORCH_ASSERT_ONLY_METHOD_OPERATORS
#endif

// @generated by torchgen/gen_backend_stubs.py from RegisterDispatchKey.cpp

#include <c10/core/TensorImpl.h>
#include <c10/core/Allocator.h>
#include <ATen/DeviceGuard.h>
#include <ATen/NamedTensorUtils.h>
#include <ATen/Utils.h>
#include <ATen/WrapDimUtils.h>
#include <ATen/Dispatch.h>
#include <c10/util/ExclusivelyOwned.h>
#include <c10/util/Half.h>
#include <c10/core/UndefinedTensorImpl.h>
#include <optional>
#include <ATen/Tensor.h>
#include <ATen/native/Resize.h>

#include <cstddef>
#include <functional>
#include <memory>
#include <utility>

#include <ATen/Config.h>
#include <ATen/core/op_registration/adaption.h>
#include <torch/library.h>

#include "QuantizedXPUNativeFunctions.h"
#include <ATen/NativeFunctions.h>
#include <ATen/Functions.h>
#include <c10/macros/Macros.h>
#include <ATen/Functions.h>

// See template file RegisterDispatchDefinitions.ini
namespace at {
// NB: TORCH_LIBRARY_IMPL must be in an anonymous namespace to avoid
// ambiguity with conflicting identifiers that may have been defined in
// at namespace already.
namespace {
C10_DIAGNOSTIC_PUSH_AND_IGNORED_IF_DEFINED("-Wunused-function")
void resize_out(const Tensor &out, IntArrayRef sizes, IntArrayRef strides, const TensorOptions &options) {
  TORCH_CHECK(options.dtype() == out.dtype(),
      "Expected out tensor to have dtype ", options.dtype(), ", but got ", out.dtype(), " instead");
  TORCH_CHECK(options.device() == out.device(),
      "Expected out tensor to have device ", options.device(), ", but got ", out.device(), " instead");
  const bool resized = at::native::resize_output(out, sizes);
  // Only restride if a resize occurred; otherwise we ignore the (advisory)
  // strides from the meta function and directly use the output tensor's
  // preexisting strides
  if (resized) {
    if (!strides.empty()) {
      TORCH_INTERNAL_ASSERT(!options.memory_format_opt().has_value());
      // TODO: avoid the redispatch here
      out.as_strided_(sizes, strides);
    } else if (options.memory_format_opt().has_value()) {
      out.unsafeGetTensorImpl()->empty_tensor_restride(*options.memory_format_opt());
    }
  }
}
void check_inplace(const Tensor &self, IntArrayRef sizes, const TensorOptions &options) {
  // These checks are needed on those operators that:
  //   1) don't use 'TensorIterator' (e.g. 'addmm' and 'baddbmm')
  //   2) have particular typing rules (e.g. 'cumsum' and 'cumprod')
  // For other operators (e.g. 'add'), 'TensorIterator' already checks
  // these things separately.
  TORCH_CHECK(options.dtype() == self.dtype(),
      "Bad in-place call: ",
      "input tensor dtype ", self.dtype(), " and output tensor dtype ", options.dtype(), " should match");
  TORCH_CHECK(options.device() == self.device(),
      "Bad in-place call: ",
      "input tensor device ", self.device(), " and output tensor device ", options.device(), " should match");
  TORCH_CHECK(sizes == self.sizes(),
      "Bad in-place call: ",
      "input tensor size ", self.sizes(), " and output tensor size ", sizes, " should match");
}
C10_DIAGNOSTIC_POP()
namespace {
at::Tensor wrapper_QuantizedXPU__as_strided(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<c10::SymInt> storage_offset) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::as_strided(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), storage_offset.has_value() ? ::std::make_optional(storage_offset->guard_int(__FILE__, __LINE__)) : ::std::nullopt);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU__copy_(at::Tensor & self, const at::Tensor & src, bool non_blocking) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::copy_(self, src, non_blocking);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU_memory_format_empty(c10::SymIntArrayRef size, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, ::std::optional<at::MemoryFormat> memory_format) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  
  const DeviceGuard device_guard(device_or_default(device));
  return at::AtenIpexTypeQuantizedXPU::empty(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU___empty_affine_quantized(c10::SymIntArrayRef size, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, double scale, int64_t zero_point, ::std::optional<at::MemoryFormat> memory_format) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  
  const DeviceGuard device_guard(device_or_default(device));
  return at::AtenIpexTypeQuantizedXPU::_empty_affine_quantized(C10_AS_INTARRAYREF_SLOW(size), dtype, layout, device, pin_memory, scale, zero_point, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU___empty_per_channel_affine_quantized(c10::SymIntArrayRef size, const at::Tensor & scales, const at::Tensor & zero_points, int64_t axis, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, ::std::optional<at::MemoryFormat> memory_format) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  
  const DeviceGuard device_guard(device_or_default(device));
  return at::AtenIpexTypeQuantizedXPU::_empty_per_channel_affine_quantized(C10_AS_INTARRAYREF_SLOW(size), scales, zero_points, axis, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
const at::Tensor & wrapper_QuantizedXPU__resize_(const at::Tensor & self, c10::SymIntArrayRef size, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::resize_(self, C10_AS_INTARRAYREF_SLOW(size), memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__empty_quantized(at::IntArrayRef size, const at::Tensor & qtensor, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, ::std::optional<at::MemoryFormat> memory_format) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, qtensor, "wrapper_QuantizedXPU__empty_quantized", "qtensor");
  
  const DeviceGuard device_guard(device_or_default(device));
  return at::AtenIpexTypeQuantizedXPU::empty_quantized(size, qtensor, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__empty_like(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::empty_like(self, dtype, layout, device, pin_memory, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__empty_strided(c10::SymIntArrayRef size, c10::SymIntArrayRef stride, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  
  const DeviceGuard device_guard(device_or_default(device));
  return at::AtenIpexTypeQuantizedXPU::empty_strided(C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride), dtype, layout, device, pin_memory);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__quantized_max_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, at::IntArrayRef dilation, bool ceil_mode) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__quantized_max_pool2d", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::quantized_max_pool2d(self, kernel_size, stride, padding, dilation, ceil_mode);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU___reshape_alias(const at::Tensor & self, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::_reshape_alias(self, C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__relu(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::relu(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__sigmoid(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::sigmoid(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU_out_sigmoid_out(const at::Tensor & self, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::sigmoid_out(self, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU__sigmoid_(at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::sigmoid_(self);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU__transpose_(at::Tensor & self, int64_t dim0, int64_t dim1) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::transpose_(self, dim0, dim1);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__clone(const at::Tensor & self, ::std::optional<at::MemoryFormat> memory_format) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__clone", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::clone(self, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__addmm(const at::Tensor & self, const at::Tensor & mat1, const at::Tensor & mat2, const at::Scalar & beta, const at::Scalar & alpha) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__addmm", "self");
  c10::impl::check_and_update_common_device(common_device, mat1, "wrapper_QuantizedXPU__addmm", "mat1");
  c10::impl::check_and_update_common_device(common_device, mat2, "wrapper_QuantizedXPU__addmm", "mat2");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::addmm(self, mat1, mat2, beta, alpha);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU_self_dequantize(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU_self_dequantize", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::dequantize(self);
}
} // anonymous namespace
namespace {
double wrapper_QuantizedXPU__q_scale(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__q_scale", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::q_scale(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_QuantizedXPU__q_zero_point(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__q_zero_point", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::q_zero_point(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__q_per_channel_scales(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__q_per_channel_scales", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::q_per_channel_scales(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__q_per_channel_zero_points(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__q_per_channel_zero_points", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::q_per_channel_zero_points(self);
}
} // anonymous namespace
namespace {
int64_t wrapper_QuantizedXPU__q_per_channel_axis(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__q_per_channel_axis", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::q_per_channel_axis(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__int_repr(const at::Tensor & self) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::int_repr(self);
}
} // anonymous namespace
namespace {
at::QScheme wrapper_QuantizedXPU__qscheme(const at::Tensor & self) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__qscheme", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::qscheme(self);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU___to_copy(const at::Tensor & self, ::std::optional<at::ScalarType> dtype, ::std::optional<at::Layout> layout, ::std::optional<at::Device> device, ::std::optional<bool> pin_memory, bool non_blocking, ::std::optional<at::MemoryFormat> memory_format) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::_to_copy(self, dtype, layout, device, pin_memory, non_blocking, memory_format);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU_source_Storage_storage_offset_set_(at::Tensor & self, at::Storage source, c10::SymInt storage_offset, c10::SymIntArrayRef size, c10::SymIntArrayRef stride) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::set_(self, source, storage_offset.guard_int(__FILE__, __LINE__), C10_AS_INTARRAYREF_SLOW(size), C10_AS_INTARRAYREF_SLOW(stride));
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__view(const at::Tensor & self, c10::SymIntArrayRef size) {
    // No device check
  // DeviceGuard omitted
  return at::AtenIpexTypeQuantizedXPU::view(self, C10_AS_INTARRAYREF_SLOW(size));
}
} // anonymous namespace
namespace {
bool wrapper_QuantizedXPU__equal(const at::Tensor & self, const at::Tensor & other) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__equal", "self");
  c10::impl::check_and_update_common_device(common_device, other, "wrapper_QuantizedXPU__equal", "other");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::equal(self, other);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__hardtanh(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::hardtanh(self, min_val, max_val);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU_out_hardtanh_out(const at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val, at::Tensor & out) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::hardtanh_out(self, min_val, max_val, out);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU__hardtanh_(at::Tensor & self, const at::Scalar & min_val, const at::Scalar & max_val) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::hardtanh_(self, min_val, max_val);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__leaky_relu(const at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::leaky_relu(self, negative_slope);
}
} // anonymous namespace
namespace {
at::Tensor & wrapper_QuantizedXPU__leaky_relu_(at::Tensor & self, const at::Scalar & negative_slope) {
    // No device check
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::leaky_relu_(self, negative_slope);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__avg_pool2d(const at::Tensor & self, at::IntArrayRef kernel_size, at::IntArrayRef stride, at::IntArrayRef padding, bool ceil_mode, bool count_include_pad, ::std::optional<int64_t> divisor_override) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__avg_pool2d", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::avg_pool2d(self, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU_vec_upsample_nearest3d(const at::Tensor & input, at::OptionalSymIntArrayRef output_size, ::std::optional<at::ArrayRef<double>> scale_factors) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, input, "wrapper_QuantizedXPU_vec_upsample_nearest3d", "input");
  const OptionalDeviceGuard device_guard(device_of(input));
  return at::AtenIpexTypeQuantizedXPU::upsample_nearest3d(input, output_size.has_value() ? ::std::make_optional(C10_AS_INTARRAYREF_SLOW(*output_size)) : ::std::nullopt, scale_factors);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__upsample_nearest2d(const at::Tensor & self, c10::SymIntArrayRef output_size, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__upsample_nearest2d", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::upsample_nearest2d(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_h, scales_w);
}
} // anonymous namespace
namespace {
at::Tensor wrapper_QuantizedXPU__upsample_nearest3d(const at::Tensor & self, c10::SymIntArrayRef output_size, ::std::optional<double> scales_d, ::std::optional<double> scales_h, ::std::optional<double> scales_w) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__upsample_nearest3d", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::upsample_nearest3d(self, C10_AS_INTARRAYREF_SLOW(output_size), scales_d, scales_h, scales_w);
}
} // anonymous namespace
namespace {
void wrapper_QuantizedXPU__record_stream(at::Tensor & self, at::Stream s) {
  std::optional<Device> common_device = std::nullopt;
(void)common_device; // Suppress unused variable warning
  c10::impl::check_and_update_common_device(common_device, self, "wrapper_QuantizedXPU__record_stream", "self");
  const OptionalDeviceGuard device_guard(device_of(self));
  return at::AtenIpexTypeQuantizedXPU::record_stream(self, s);
}
} // anonymous namespace
TORCH_LIBRARY_IMPL(aten, QuantizedXPU, m) {
    m.impl("as_strided",
    TORCH_FN(wrapper_QuantizedXPU__as_strided));
    m.impl("copy_",
    TORCH_FN(wrapper_QuantizedXPU__copy_));
    m.impl("empty.memory_format",
    TORCH_FN(wrapper_QuantizedXPU_memory_format_empty));
    m.impl("_empty_affine_quantized",
    TORCH_FN(wrapper_QuantizedXPU___empty_affine_quantized));
    m.impl("_empty_per_channel_affine_quantized",
    TORCH_FN(wrapper_QuantizedXPU___empty_per_channel_affine_quantized));
    m.impl("resize_",
    TORCH_FN(wrapper_QuantizedXPU__resize_));
    m.impl("empty_quantized",
    TORCH_FN(wrapper_QuantizedXPU__empty_quantized));
    m.impl("empty_like",
    TORCH_FN(wrapper_QuantizedXPU__empty_like));
    m.impl("empty_strided",
    TORCH_FN(wrapper_QuantizedXPU__empty_strided));
    m.impl("quantized_max_pool2d",
    TORCH_FN(wrapper_QuantizedXPU__quantized_max_pool2d));
    m.impl("_reshape_alias",
    TORCH_FN(wrapper_QuantizedXPU___reshape_alias));
    m.impl("relu",
    TORCH_FN(wrapper_QuantizedXPU__relu));
    m.impl("sigmoid",
    TORCH_FN(wrapper_QuantizedXPU__sigmoid));
    m.impl("sigmoid.out",
    TORCH_FN(wrapper_QuantizedXPU_out_sigmoid_out));
    m.impl("sigmoid_",
    TORCH_FN(wrapper_QuantizedXPU__sigmoid_));
    m.impl("transpose_",
    TORCH_FN(wrapper_QuantizedXPU__transpose_));
    m.impl("clone",
    TORCH_FN(wrapper_QuantizedXPU__clone));
    m.impl("addmm",
    TORCH_FN(wrapper_QuantizedXPU__addmm));
    m.impl("dequantize.self",
    TORCH_FN(wrapper_QuantizedXPU_self_dequantize));
    m.impl("q_scale",
    TORCH_FN(wrapper_QuantizedXPU__q_scale));
    m.impl("q_zero_point",
    TORCH_FN(wrapper_QuantizedXPU__q_zero_point));
    m.impl("q_per_channel_scales",
    TORCH_FN(wrapper_QuantizedXPU__q_per_channel_scales));
    m.impl("q_per_channel_zero_points",
    TORCH_FN(wrapper_QuantizedXPU__q_per_channel_zero_points));
    m.impl("q_per_channel_axis",
    TORCH_FN(wrapper_QuantizedXPU__q_per_channel_axis));
    m.impl("int_repr",
    TORCH_FN(wrapper_QuantizedXPU__int_repr));
    m.impl("qscheme",
    TORCH_FN(wrapper_QuantizedXPU__qscheme));
    m.impl("_to_copy",
    TORCH_FN(wrapper_QuantizedXPU___to_copy));
    m.impl("set_.source_Storage_storage_offset",
    TORCH_FN(wrapper_QuantizedXPU_source_Storage_storage_offset_set_));
    m.impl("view",
    TORCH_FN(wrapper_QuantizedXPU__view));
    m.impl("equal",
    TORCH_FN(wrapper_QuantizedXPU__equal));
    m.impl("hardtanh",
    TORCH_FN(wrapper_QuantizedXPU__hardtanh));
    m.impl("hardtanh.out",
    TORCH_FN(wrapper_QuantizedXPU_out_hardtanh_out));
    m.impl("hardtanh_",
    TORCH_FN(wrapper_QuantizedXPU__hardtanh_));
    m.impl("leaky_relu",
    TORCH_FN(wrapper_QuantizedXPU__leaky_relu));
    m.impl("leaky_relu_",
    TORCH_FN(wrapper_QuantizedXPU__leaky_relu_));
//    m.impl("_adaptive_avg_pool2d",
//    TORCH_FN(wrapper_QuantizedXPU___adaptive_avg_pool2d));
    m.impl("avg_pool2d",
    TORCH_FN(wrapper_QuantizedXPU__avg_pool2d));
    m.impl("upsample_nearest3d.vec",
    TORCH_FN(wrapper_QuantizedXPU_vec_upsample_nearest3d));
    m.impl("upsample_nearest2d",
    TORCH_FN(wrapper_QuantizedXPU__upsample_nearest2d));
    m.impl("upsample_nearest3d",
    TORCH_FN(wrapper_QuantizedXPU__upsample_nearest3d));
    m.impl("record_stream",
    TORCH_FN(wrapper_QuantizedXPU__record_stream));
};
} // anonymous namespace
namespace quantizedxpu {
} // namespace quantizedxpu
} // namespace at
