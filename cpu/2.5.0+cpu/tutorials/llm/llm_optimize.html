

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>LLM Optimizations Frontend API &mdash; Intel&amp;#174 Extension for PyTorch* 2.5.0+cpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=01a6a0bb" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Performance" href="../performance.html" />
    <link rel="prev" title="Large Language Models (LLM) Optimization Overview" href="../llm.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">LLM Optimizations Frontend API</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#pseudocode-of-common-usage-scenarios">Pseudocode of Common Usage Scenarios</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#fp32-bf16">FP32/BF16</a></li>
<li class="toctree-l4"><a class="reference internal" href="#smoothquant">SmoothQuant</a></li>
<li class="toctree-l4"><a class="reference internal" href="#weight-only-quantization-woq">Weight Only Quantization (WOQ)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-inference-with-deepspeed">Distributed Inference with DeepSpeed</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#ipex-llm-optimized-model-list-for-inference"><cite>ipex.llm</cite> Optimized Model List for Inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#module-level-optimization-api-for-customized-llm-prototype">Module Level Optimization API for customized LLM (Prototype)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#demos">Demos</a></li>
<li class="toctree-l2"><a class="reference internal" href="../llm.html#optimization-methodologies">Optimization Methodologies</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../llm.html">Large Language Models (LLM) Optimization Overview</a></li>
      <li class="breadcrumb-item active">LLM Optimizations Frontend API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/llm/llm_optimize.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="llm-optimizations-frontend-api">
<h1>LLM Optimizations Frontend API<a class="headerlink" href="#llm-optimizations-frontend-api" title="Link to this heading"></a></h1>
<p>The new API function, <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code>, is designed to optimize transformer-based models within frontend Python modules, with a particular focus on Large Language Models (LLMs).
It provides optimizations for both model-wise and content-generation-wise.
You just need to invoke the <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code> function instead of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> function to apply all optimizations transparently.</p>
<p>This API currently supports for inference workloads of certain models.
API documentation is available at <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/api_doc.html#ipex.llm.optimize">API Docs page</a>,
and supported model list can be found at <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/llm.html#ipexllm-optimized-model-list-for-inference">this page</a>.</p>
<p>For LLM fine-tuning, please check the <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/llm/fine-tuning">LLM fine-tuning tutorial</a>.</p>
<section id="pseudocode-of-common-usage-scenarios">
<h2>Pseudocode of Common Usage Scenarios<a class="headerlink" href="#pseudocode-of-common-usage-scenarios" title="Link to this heading"></a></h2>
<p>The following sections show pseudocode snippets to invoke Intel® Extension for PyTorch* APIs to work with LLM models.
Complete examples can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/llm/inference">the Example directory</a>.</p>
<section id="fp32-bf16">
<h3>FP32/BF16<a class="headerlink" href="#fp32-bf16" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">model</span><span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="c1"># or torch.bfloat16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># inference with model.generate()</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
<section id="smoothquant">
<h3>SmoothQuant<a class="headerlink" href="#smoothquant" title="Link to this heading"></a></h3>
<p>Supports INT8.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="c1">#################### code changes ####################  # noqa F401</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">from</span> <span class="nn">intel_extension_for_pytorch.quantization</span> <span class="kn">import</span> <span class="n">prepare</span>
<span class="c1">######################################################  # noqa F401</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="c1"># load model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="o">...</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="c1">#################### code changes ####################  # noqa F401</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_smooth_quant_qconfig_mapping</span><span class="p">()</span>
<span class="c1"># stage 1: calibration</span>
<span class="c1"># prepare your calibration dataset samples</span>
<span class="n">calib_dataset</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">your_calibration_dataset</span><span class="p">)</span>
<span class="n">example_inputs</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># get one sample input from calib_samples</span>
<span class="n">calibration_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span>
  <span class="n">quantization_config</span><span class="o">=</span><span class="n">qconfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span>
  <span class="n">calibration_model</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <span class="n">qconfig</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span>
<span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">for</span> <span class="n">calib_samples</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">calib_dataset</span><span class="p">):</span>
    <span class="n">prepared_model</span><span class="p">(</span><span class="n">calib_samples</span><span class="p">)</span>
<span class="n">prepared_model</span><span class="o">.</span><span class="n">save_qconf_summary</span><span class="p">(</span><span class="n">qconf_summary</span><span class="o">=</span><span class="n">qconfig_summary_file_path</span><span class="p">)</span>

<span class="c1"># stage 2: quantization</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span>
  <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span>
  <span class="n">quantization_config</span><span class="o">=</span><span class="n">qconfig</span><span class="p">,</span>
  <span class="n">qconfig_summary_file</span><span class="o">=</span><span class="n">qconfig_summary_file_path</span><span class="p">,</span>
<span class="p">)</span>
<span class="c1">######################################################  # noqa F401</span>

<span class="c1"># generation inference loop</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">inference_mode</span><span class="p">():</span>
    <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">({</span><span class="n">your</span> <span class="n">generate</span> <span class="n">parameters</span><span class="p">})</span>
</pre></div>
</div>
</section>
<section id="weight-only-quantization-woq">
<h3>Weight Only Quantization (WOQ)<a class="headerlink" href="#weight-only-quantization-woq" title="Link to this heading"></a></h3>
<p>Supports INT8 and INT4.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">model</span><span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">get_weight_only_quant_qconfig_mapping</span><span class="p">(</span>
  <span class="n">weight_dtype</span><span class="o">=</span><span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">WoqWeightDtype</span><span class="o">.</span><span class="n">INT8</span><span class="p">,</span> <span class="c1"># or INT4/NF4</span>
  <span class="n">lowp_mode</span><span class="o">=</span><span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">WoqLowpMode</span><span class="o">.</span><span class="n">NONE</span><span class="p">,</span> <span class="c1"># or FP16, BF16, INT8</span>
<span class="p">)</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># optionally load int4 or int8 checkpoint</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">qconfig</span><span class="p">,</span> <span class="n">low_precision_checkpoint</span><span class="o">=</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># inference with model.generate()</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
<section id="distributed-inference-with-deepspeed">
<h3>Distributed Inference with DeepSpeed<a class="headerlink" href="#distributed-inference-with-deepspeed" title="Link to this heading"></a></h3>
<p>Distributed inference can be performed with <code class="docutils literal notranslate"><span class="pre">DeepSpeed</span></code>. Based on original Intel® Extension for PyTorch* scripts, the following code changes are required.</p>
<p>Check <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/main/examples/cpu/llm/inference/distributed">LLM distributed inference examples</a> for complete codes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">intel_extension_for_pytorch</span> <span class="k">as</span> <span class="nn">ipex</span>
<span class="kn">import</span> <span class="nn">deepspeed</span>
<span class="kn">import</span> <span class="nn">transformers</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="c1"># or torch.bfloat16</span>
<span class="n">deepspeed</span><span class="o">.</span><span class="n">init_distributed</span><span class="p">(</span><span class="n">deepspeed</span><span class="o">.</span><span class="n">accelerator</span><span class="o">.</span><span class="n">get_accelerator</span><span class="p">()</span><span class="o">.</span><span class="n">communication_backend_name</span><span class="p">())</span>

<span class="n">world_size</span> <span class="o">=</span> <span class="o">...</span> <span class="c1"># get int from env var &quot;WORLD_SIZE&quot; or &quot;PMI_SIZE&quot;</span>
<span class="k">with</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">OnDevice</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;meta&quot;</span><span class="p">):</span>
  <span class="n">model</span><span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">deepspeed</span><span class="o">.</span><span class="n">init_inference</span><span class="p">(</span>
  <span class="n">model</span><span class="p">,</span>
  <span class="n">mp_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
  <span class="n">base_dir</span><span class="o">=</span><span class="n">repo_root</span><span class="p">,</span>
  <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span>
  <span class="n">checkpoint</span><span class="o">=</span><span class="n">checkpoints_json</span><span class="p">,</span>
  <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">module</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="c1"># inference</span>
<span class="o">...</span>
</pre></div>
</div>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../llm.html" class="btn btn-neutral float-left" title="Large Language Models (LLM) Optimization Overview" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../performance.html" class="btn btn-neutral float-right" title="Performance" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x74cfa9839ed0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a> <a href="/#" data-wap_ref="dns" id="wap_dns"><small>| Your Privacy Choices</small></a> <a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref="nac" id="wap_nac"><small>| Notice at Collection</small></a> </div> <p></p> <div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>. </div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>