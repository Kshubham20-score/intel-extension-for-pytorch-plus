import torch
import intel_extension_for_pytorch  # noqa

from torch.testing._internal.common_utils import TestCase
import pytest


# torch.set_printoptions(profile="full")

b = 2
n = 4
seq_len = 32
head_size = 64


class TestTorchMethod(TestCase):
    @pytest.mark.skipif(not torch.xpu.has_xetla(), reason="fallback is required")
    def test_sdp_backward(self, dtype=torch.bfloat16):
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states,
            key=key_states,
            value=value_states,
        )

        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states_xpu,
            key=key_states_xpu,
            value=value_states_xpu,
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)

        self.assertEqual(
            query_states.grad, query_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            key_states.grad, key_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            value_states.grad, value_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )

    @pytest.mark.skipif(not torch.xpu.has_xetla(), reason="fallback is required")
    def test_sdp_backward_with_bias(self, dtype=torch.bfloat16):
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        bias = torch.randn((b, 1, seq_len, seq_len))

        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        bias_xpu = bias.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)
        bias.requires_grad_(True)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        bias_xpu.requires_grad_(True)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query_states, key_states, value_states, bias
        )
        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query_states_xpu, key_states_xpu, value_states_xpu, bias_xpu
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)

        self.assertEqual(
            query_states.grad, query_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            key_states.grad, key_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            value_states.grad, value_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(bias.grad, bias_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-1)

    @pytest.mark.skipif(not torch.xpu.has_xetla(), reason="fallback is required")
    def test_sdp_backward_with_bias_no_grad(self, dtype=torch.bfloat16):
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        bias = torch.randn((b, 1, seq_len, seq_len))

        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        bias_xpu = bias.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)
        bias.requires_grad_(False)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        bias_xpu.requires_grad_(False)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query_states, key_states, value_states, bias
        )
        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query_states_xpu, key_states_xpu, value_states_xpu, bias_xpu
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)

        self.assertEqual(
            query_states.grad, query_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            key_states.grad, key_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            value_states.grad, value_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )

    @pytest.mark.skipif(not torch.xpu.has_xetla(), reason="fallback is required")
    def test_sdp_backward_with_bias_512(self, dtype=torch.bfloat16):
        head_size = 512
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        bias = torch.randn((b, 1, seq_len, seq_len))

        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        bias_xpu = bias.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)
        bias.requires_grad_(True)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        bias_xpu.requires_grad_(True)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query_states, key_states, value_states, bias
        )
        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query_states_xpu, key_states_xpu, value_states_xpu, bias_xpu
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)

        self.assertEqual(r_cpu, r_xpu.float(), atol=1e-2, rtol=1e-2)

        # grad has a bit of elements mismatch (less than 1%), so we don't check here

    @pytest.mark.skipif(not torch.xpu.has_xetla(), reason="fallback is required")
    def test_sdp_backward_causal(self, dtype=torch.bfloat16):
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states, key=key_states, value=value_states, is_causal=True
        )

        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states_xpu,
            key=key_states_xpu,
            value=value_states_xpu,
            is_causal=True,
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)

        self.assertEqual(
            query_states.grad, query_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            key_states.grad, key_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            value_states.grad, value_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )

    @pytest.mark.skipif(True, reason="dropout is random")
    def test_sdp_backward_dropout(self, dtype=torch.bfloat16):
        query_states = torch.randn((b, n, seq_len, head_size))
        key_states = torch.randn((b, n, seq_len, head_size))
        value_states = torch.randn((b, n, seq_len, head_size))
        bias = torch.randn((b, 1, seq_len, seq_len))
        grad = torch.randn((b, n, seq_len, head_size))

        query_states_xpu = query_states.bfloat16().xpu()
        key_states_xpu = key_states.bfloat16().xpu()
        value_states_xpu = value_states.bfloat16().xpu()
        grad_xpu = grad.bfloat16().xpu()

        query_states.requires_grad_(True)
        key_states.requires_grad_(True)
        value_states.requires_grad_(True)

        query_states_xpu.requires_grad_(True)
        key_states_xpu.requires_grad_(True)
        value_states_xpu.requires_grad_(True)
        r_cpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states, key=key_states, value=value_states, dropout_p=0.5
        )

        r_xpu = torch.nn.functional.scaled_dot_product_attention(
            query=query_states_xpu,
            key=key_states_xpu,
            value=value_states_xpu,
            dropout_p=0.5,
        )
        r_cpu.backward(grad)
        r_xpu.backward(grad_xpu)
        print(r_xpu.cpu())

        self.assertEqual(
            query_states.grad, query_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            key_states.grad, key_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
        self.assertEqual(
            value_states.grad, value_states_xpu.grad.cpu().float(), atol=1e-2, rtol=1e-2
        )
