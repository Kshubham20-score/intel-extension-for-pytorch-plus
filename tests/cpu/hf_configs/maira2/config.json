{
    "architectures": [
        "Maira2ForConditionalGeneration"
    ],
    "auto_map": {
        "AutoConfig": "configuration_maira2.Maira2Config",
        "AutoModelForCausalLM": "modeling_maira2.Maira2ForConditionalGeneration",
        "AutoModelForVision2Seq": "modeling_maira2.Maira2ForConditionalGeneration"
    },
    "hidden_size": 4096,
    "ignore_index": -100,
    "image_seq_length": 576,
    "image_token_index": 32204,
    "model_type": "maira2",
    "pad_token_id": 0,
    "projector_hidden_act": "gelu",
    "projector_n_layers": 1,
    "text_config": {
        "_name_or_path": "microsoft/Phi-3-mini-4k-instruct",
        "add_cross_attention": false,
        "architectures": [
            "Phi3ForCausalLM"
        ],
        "attention_dropout": 0.0,
        "bad_words_ids": null,
        "begin_suppress_tokens": null,
        "bos_token_id": 1,
        "chunk_size_feed_forward": 0,
        "cross_attention_hidden_size": null,
        "decoder_start_token_id": null,
        "diversity_penalty": 0.0,
        "do_sample": false,
        "early_stopping": false,
        "embd_pdrop": 0.0,
        "encoder_no_repeat_ngram_size": 0,
        "eos_token_id": 32000,
        "exponential_decay_length_penalty": null,
        "finetuning_task": null,
        "forced_bos_token_id": null,
        "forced_eos_token_id": null,
        "hidden_act": "silu",
        "hidden_size": 3072,
        "id2label": {
            "0": "LABEL_0",
            "1": "LABEL_1"
        },
        "initializer_range": 0.02,
        "intermediate_size": 8192,
        "is_decoder": false,
        "is_encoder_decoder": false,
        "label2id": {
            "LABEL_0": 0,
            "LABEL_1": 1
        },
        "length_penalty": 1.0,
        "max_length": 20,
        "max_position_embeddings": 4096,
        "min_length": 0,
        "model_type": "phi3",
        "no_repeat_ngram_size": 0,
        "num_attention_heads": 32,
        "num_beam_groups": 1,
        "num_beams": 1,
        "num_hidden_layers": 1,
        "num_key_value_heads": 32,
        "num_return_sequences": 1,
        "original_max_position_embeddings": 4096,
        "output_attentions": false,
        "output_hidden_states": false,
        "output_scores": false,
        "pad_token_id": 32000,
        "prefix": null,
        "problem_type": null,
        "pruned_heads": {},
        "remove_invalid_values": false,
        "repetition_penalty": 1.0,
        "resid_pdrop": 0.0,
        "return_dict": true,
        "return_dict_in_generate": false,
        "rms_norm_eps": 1e-05,
        "rope_scaling": {
            "long_factor": [
                1.0,
                1.0638297872340425,
                1.127659574468085,
                1.1914893617021276,
                1.2553191489361701,
                1.3191489361702127,
                1.3829787234042552,
                1.4468085106382977,
                1.5106382978723403,
                1.5744680851063828,
                1.6382978723404253,
                1.7021276595744679,
                1.7659574468085106,
                1.8297872340425532,
                1.8936170212765957,
                1.9574468085106382,
                2.0212765957446805,
                2.085106382978723,
                2.1489361702127656,
                2.212765957446808,
                2.2765957446808507,
                2.340425531914893,
                2.4042553191489358,
                2.4680851063829783,
                2.5319148936170213,
                2.595744680851064,
                2.6595744680851063,
                2.723404255319149,
                2.7872340425531914,
                2.851063829787234,
                2.9148936170212765,
                2.978723404255319,
                3.0425531914893615,
                3.106382978723404,
                3.1702127659574466,
                3.234042553191489,
                3.2978723404255317,
                3.361702127659574,
                3.4255319148936167,
                3.4893617021276593,
                3.553191489361702,
                3.6170212765957444,
                3.680851063829787,
                3.7446808510638294,
                3.808510638297872,
                3.8723404255319145,
                3.936170212765957,
                4.0
            ],
            "short_factor": [
                1.0,
                1.0212765957446808,
                1.0425531914893618,
                1.0638297872340425,
                1.0851063829787233,
                1.1063829787234043,
                1.127659574468085,
                1.148936170212766,
                1.1702127659574468,
                1.1914893617021276,
                1.2127659574468086,
                1.2340425531914894,
                1.2553191489361701,
                1.2765957446808511,
                1.297872340425532,
                1.3191489361702127,
                1.3404255319148937,
                1.3617021276595744,
                1.3829787234042552,
                1.4042553191489362,
                1.425531914893617,
                1.4468085106382977,
                1.4680851063829787,
                1.4893617021276595,
                1.5106382978723403,
                1.5319148936170213,
                1.5531914893617023,
                1.5744680851063828,
                1.5957446808510638,
                1.6170212765957448,
                1.6382978723404256,
                1.6595744680851063,
                1.6808510638297873,
                1.702127659574468,
                1.7234042553191489,
                1.7446808510638299,
                1.7659574468085106,
                1.7872340425531914,
                1.8085106382978724,
                1.8297872340425532,
                1.851063829787234,
                1.872340425531915,
                1.8936170212765957,
                1.9148936170212765,
                1.9361702127659575,
                1.9574468085106382,
                1.978723404255319,
                2.0
            ],
            "type": "longrope"
        },
        "rope_theta": 10000.0,
        "sep_token_id": null,
        "sliding_window": 2047,
        "suppress_tokens": null,
        "task_specific_params": null,
        "temperature": 1.0,
        "tf_legacy_loss": false,
        "tie_encoder_decoder": false,
        "tie_word_embeddings": false,
        "tokenizer_class": null,
        "top_k": 50,
        "top_p": 1.0,
        "torch_dtype": "float16",
        "torchscript": false,
        "typical_p": 1.0,
        "use_bfloat16": false,
        "use_cache": true,
        "vocab_size": 32218
    },
    "torch_dtype": "float32",
    "transformers_version": "4.46.0.dev0",
    "vision_config": {
        "_name_or_path": "",
        "add_cross_attention": false,
        "apply_layernorm": true,
        "architectures": [
            "Dinov2Model"
        ],
        "attention_probs_dropout_prob": 0.0,
        "bad_words_ids": null,
        "begin_suppress_tokens": null,
        "bos_token_id": null,
        "chunk_size_feed_forward": 0,
        "cross_attention_hidden_size": null,
        "decoder_start_token_id": null,
        "diversity_penalty": 0.0,
        "do_sample": false,
        "drop_path_rate": 0.0,
        "early_stopping": false,
        "encoder_no_repeat_ngram_size": 0,
        "eos_token_id": null,
        "exponential_decay_length_penalty": null,
        "finetuning_task": null,
        "forced_bos_token_id": null,
        "forced_eos_token_id": null,
        "hidden_act": "gelu",
        "hidden_dropout_prob": 0.0,
        "hidden_size": 768,
        "id2label": {
            "0": "LABEL_0",
            "1": "LABEL_1"
        },
        "image_size": 518,
        "initializer_range": 0.02,
        "is_decoder": false,
        "is_encoder_decoder": false,
        "label2id": {
            "LABEL_0": 0,
            "LABEL_1": 1
        },
        "layer_norm_eps": 1e-06,
        "layerscale_value": 1.0,
        "length_penalty": 1.0,
        "max_length": 20,
        "min_length": 0,
        "mlp_ratio": 4,
        "model_type": "dinov2",
        "no_repeat_ngram_size": 0,
        "num_attention_heads": 12,
        "num_beam_groups": 1,
        "num_beams": 1,
        "num_channels": 3,
        "num_hidden_layers": 12,
        "num_return_sequences": 1,
        "out_features": [
            "stage12"
        ],
        "out_indices": [
            12
        ],
        "output_attentions": false,
        "output_hidden_states": false,
        "output_scores": false,
        "pad_token_id": null,
        "patch_size": 14,
        "prefix": null,
        "problem_type": null,
        "pruned_heads": {},
        "qkv_bias": true,
        "remove_invalid_values": false,
        "repetition_penalty": 1.0,
        "reshape_hidden_states": false,
        "return_dict": true,
        "return_dict_in_generate": false,
        "sep_token_id": null,
        "stage_names": [
            "stem",
            "stage1",
            "stage2",
            "stage3",
            "stage4",
            "stage5",
            "stage6",
            "stage7",
            "stage8",
            "stage9",
            "stage10",
            "stage11",
            "stage12"
        ],
        "suppress_tokens": null,
        "task_specific_params": null,
        "temperature": 1.0,
        "tf_legacy_loss": false,
        "tie_encoder_decoder": false,
        "tie_word_embeddings": true,
        "tokenizer_class": null,
        "top_k": 50,
        "top_p": 1.0,
        "torch_dtype": "float32",
        "torchscript": false,
        "typical_p": 1.0,
        "use_bfloat16": false,
        "use_swiglu_ffn": false
    },
    "vision_feature_layer": -1,
    "vision_feature_select_strategy": "default"
}