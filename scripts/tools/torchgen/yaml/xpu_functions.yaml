backend: XPU
cpp_namespace: at
class_name: AtenIpexTypeXPU
use_out_as_primary: true
device_guard: true
supported:
####################################
# Here list these ops conflicted with torch-xpu-ops fallback. We have to mute
# them here and register them with IPEX_TORCH_LIBRARY_IMPL to suppress the
# previously registered kernel overriding warning message.
#
# - angle
# - angle.out
################## override below ops due to UTs failed
# - _embedding_bag
# - _embedding_bag_forward_only
# - glu_backward
# - glu_backward.grad_input
# - index_select
# - index_select.out
# - index_copy_
# - fractional_max_pool2d
# - fractional_max_pool3d
# - special_erfcx
# - special_ndtri
################## override below ops due to performance issues
# - convolution_overrideable
# - convolution_backward_overrideable
# - _addmm_activation.out
# - addmm.out
# - addmv.out
# - mm
# - mm.out
# - baddbmm
# - baddbmm.out
# - baddbmm_
# - addbmm
# - addbmm_
# - addbmm.out
# - bmm
# - bmm.out
# - tensordot.out
# - native_batch_norm
# - native_batch_norm.out
# - native_batch_norm_backward
# - upsample_bilinear2d
# - upsample_bilinear2d.out
# - upsample_bilinear2d_backward
# - upsample_bilinear2d_backward.grad_input
# - max_pool2d_with_indices
# - max_pool2d_with_indices.out
# - max_pool2d_with_indices_backward
# - max_pool2d_with_indices_backward.grad_input
# - _index_put_impl_
# - nonzero
# - nonzero.out
# - sum.IntList_out
# - sum.dim_IntList
# - nansum
# - nansum.out
#  - native_group_norm
#  - native_group_norm_backward
#  - addmm
#  - _addmm_activation_out
############oneMKL##################
#  - _fft_c2r
#  - _fft_r2c
#  - _fft_c2c
#  - _fft_c2r.out
#  - _fft_r2c.out
#  - _fft_c2c.out
#  - linalg_qr.out
#  - _lu_with_info
#  - _linalg_svd.U
#  - linalg_inv_ex.inverse
#  - linalg_lu_factor_ex.out
#  - linalg_lu_solve.out
#  - linalg_lu.out
#  - linalg_matrix_exp
#  - cholesky
#  - cholesky.out
#  - cholesky_inverse
#  - cholesky_inverse.out
#  - _cholesky_solve_helper
#  - linalg_cholesky_ex.L
#  - dot
#  - geqrf
#  - geqrf.a
#  - inverse
#  - inverse.out
#  - lu_solve
#  - lu_solve.out
#  - lu_unpack
#  - lu_unpack.out
#  - ormqr
#  - ormqr.out
#  - triangular_solve
#  - triangular_solve.X
#  - linalg_eig
#  - linalg_eig.out
#  - linalg_eigh
#  - linalg_eigh.eigvals
#  - linalg_eigvalsh
#  - linalg_eigvalsh.out
#  - _linalg_eigh.eigenvalues
#  - _linalg_eigvals
#  - linalg_householder_product
#  - linalg_householder_product.out
#  - linalg_solve_triangular
#  - linalg_solve_triangular.out
#  - vdot
################## used by other ops or regesterd
#  - _compute_linear_combination
#  - _compute_linear_combination.out
#  - _fused_sdp_choice
#  - _masked_softmax
#  - _native_multi_head_attention
#  - _transform_bias_rescale_qkv
################## override due to stock pytorch performance regression
  - geometric_
  - max_pool3d_with_indices
  - max_pool3d_with_indices.out
  - max_pool3d_with_indices_backward
  - max_pool3d_with_indices_backward.grad_input
  - max_unpool2d
  - max_unpool2d.out
  - max_unpool3d
  - max_unpool3d.out
  - multilabel_margin_loss_backward
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_forward
  - multilabel_margin_loss_forward.output
####################################
  - _convert_indices_from_coo_to_csr.out
  - _convert_indices_from_csr_to_coo.out
  - _linalg_det.result
  - _linalg_slogdet.sign
  - _nested_from_padded
  - _scaled_dot_product_efficient_attention
  - _scaled_dot_product_efficient_attention_backward
  - _thnn_fused_gru_cell
  - _thnn_fused_gru_cell_backward
  - _transformer_encoder_layer_fwd
  - _validate_compressed_sparse_indices
  - _weight_int4pack_mm
  - linalg_eigvals.out
  - quantize_per_channel
  - quantize_per_tensor
  - quantize_per_tensor.tensor_qparams
  - quantize_per_tensor_dynamic
  - _make_per_channel_quantized_tensor
  - _make_per_tensor_quantized_tensor
  - std.correction_out
  - _int_mm

autograd:
  - _scaled_dot_product_attention_math
