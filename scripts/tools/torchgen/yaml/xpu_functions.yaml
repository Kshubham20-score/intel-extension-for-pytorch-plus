backend: XPU
cpp_namespace: at
class_name: AtenIpexTypeXPU
use_out_as_primary: true
device_guard: true
supported:
####################################
# Here list these ops conflicted with torch-xpu-ops fallback. We have to mute
# them here and register them with IPEX_TORCH_LIBRARY_IMPL to suppress the
# previously registered kernel overriding warning message.
#
# - angle
# - angle.out
################## override below ops due to UTs failed
# - _embedding_bag
# - _embedding_bag_forward_only
# - glu_backward
# - glu_backward.grad_input
# - index_select
# - index_select.out
# - index_copy_
# - fractional_max_pool2d
# - fractional_max_pool3d
# - special_erfcx
# - special_ndtri
################## override below ops due to performance issues
# - convolution_overrideable
# - convolution_backward_overrideable
# - _addmm_activation.out
# - addmm.out
# - addmv.out
# - mm
# - mm.out
# - baddbmm
# - baddbmm.out
# - baddbmm_
# - addbmm
# - addbmm_
# - addbmm.out
# - bmm
# - bmm.out
# - tensordot.out
# - native_batch_norm
# - native_batch_norm.out
# - native_batch_norm_backward
# - upsample_bilinear2d
# - upsample_bilinear2d.out
# - upsample_bilinear2d_backward
# - upsample_bilinear2d_backward.grad_input
# - max_pool2d_with_indices
# - max_pool2d_with_indices.out
# - max_pool2d_with_indices_backward
# - max_pool2d_with_indices_backward.grad_input
# - _index_put_impl_
# - nonzero
# - nonzero.out
# - sum.IntList_out
# - sum.dim_IntList
# - nansum
# - nansum.out
#  - native_group_norm
#  - native_group_norm_backward
#  - addmm
#  - _addmm_activation_out
############oneMKL##################
#  - _fft_c2r
#  - _fft_r2c
#  - _fft_c2c
#  - _fft_c2r.out
#  - _fft_r2c.out
#  - _fft_c2c.out
#  - linalg_qr.out
#  - _lu_with_info
#  - _linalg_svd.U
#  - linalg_inv_ex.inverse
#  - linalg_lu_factor_ex.out
#  - linalg_lu_solve.out
#  - linalg_lu.out
#  - linalg_matrix_exp
#  - cholesky
#  - cholesky.out
#  - cholesky_inverse
#  - cholesky_inverse.out
#  - _cholesky_solve_helper
#  - linalg_cholesky_ex.L
#  - dot
#  - geqrf
#  - geqrf.a
#  - inverse
#  - inverse.out
#  - lu_solve
#  - lu_solve.out
#  - lu_unpack
#  - lu_unpack.out
#  - ormqr
#  - ormqr.out
#  - triangular_solve
#  - triangular_solve.X
#  - linalg_eig
#  - linalg_eig.out
#  - linalg_eigh
#  - linalg_eigh.eigvals
#  - linalg_eigvalsh
#  - linalg_eigvalsh.out
#  - _linalg_eigh.eigenvalues
#  - _linalg_eigvals
#  - linalg_householder_product
#  - linalg_householder_product.out
#  - linalg_solve_triangular
#  - linalg_solve_triangular.out
#  - vdot
####################################
  - _convert_indices_from_coo_to_csr.out
  - _convert_indices_from_csr_to_coo.out
  - _linalg_det.result
  - _linalg_slogdet.sign
  - _linalg_solve_ex.result
  - _nested_from_padded
  - _scaled_dot_product_efficient_attention
  - _scaled_dot_product_efficient_attention_backward
  - _thnn_fused_gru_cell
  - _thnn_fused_gru_cell_backward
  - _transformer_encoder_layer_fwd
  - _unsafe_index.Tensor
  - _validate_compressed_sparse_indices
  - _weight_int4pack_mm
  - batch_norm_stats.out
  - conv_tbc
  - cross
  - cross.out
  - diag
  - diag.out
  - l1_loss
  - linalg_eigvals.out
  - pad_sequence
  - resize_as_
  - rot90
  - soft_margin_loss
  - soft_margin_loss.out
  - soft_margin_loss_backward
  - soft_margin_loss_backward.grad_input
  - square.out
  - quantize_per_channel
  - quantize_per_tensor
  - quantize_per_tensor.tensor_qparams
  - quantize_per_tensor_dynamic
  - _make_per_channel_quantized_tensor
  - _make_per_tensor_quantized_tensor
  - std.correction_out
#  - to_sparse.sparse_dim
#  - _adaptive_avg_pool3d
#  - _adaptive_avg_pool3d_backward
#  - _assert_async
#  - _assert_async.msg
#  - _cdist_backward
#  - _compute_linear_combination
#  - _compute_linear_combination.out
#  - _convert_weight_to_int4pack
#  - _ctc_loss
#  - _ctc_loss.Tensor
#  - _ctc_loss_backward
#  - _ctc_loss_backward.Tensor
#  - _cummax_helper
#  - _cummin_helper
#  - _dirichlet_grad
#  - _embedding_bag_dense_backward
#  - _embedding_bag_per_sample_weights_backward
#  - _empty_affine_quantized
#  - _empty_per_channel_affine_quantized
#  - _fake_quantize_learnable_per_channel_affine
#  - _fake_quantize_learnable_per_channel_affine_backward
#  - _fake_quantize_learnable_per_tensor_affine
#  - _fake_quantize_learnable_per_tensor_affine_backward
#  - _fake_quantize_per_tensor_affine_cachemask_tensor_qparams
#  - _foreach_abs
#  - _foreach_abs_
#  - _foreach_acos
#  - _foreach_acos_
#  - _foreach_asin
#  - _foreach_asin_
#  - _foreach_atan
#  - _foreach_atan_
#  - _foreach_ceil
#  - _foreach_ceil_
#  - _foreach_clamp_max.List
#  - _foreach_clamp_max.Scalar
#  - _foreach_clamp_max.ScalarList
#  - _foreach_clamp_max_.List
#  - _foreach_clamp_max_.Scalar
#  - _foreach_clamp_max_.ScalarList
#  - _foreach_clamp_min.List
#  - _foreach_clamp_min.Scalar
#  - _foreach_clamp_min.ScalarList
#  - _foreach_clamp_min_.List
#  - _foreach_clamp_min_.Scalar
#  - _foreach_clamp_min_.ScalarList
#  - _foreach_copy_
#  - _foreach_cos
#  - _foreach_cos_
#  - _foreach_cosh
#  - _foreach_cosh_
#  - _foreach_erf
#  - _foreach_erf_
#  - _foreach_erfc
#  - _foreach_erfc_
#  - _foreach_exp
#  - _foreach_exp_
#  - _foreach_expm1
#  - _foreach_expm1_
#  - _foreach_floor
#  - _foreach_floor_
#  - _foreach_frac
#  - _foreach_frac_
#  - _foreach_lgamma
#  - _foreach_lgamma_
#  - _foreach_log
#  - _foreach_log_
#  - _foreach_log10
#  - _foreach_log10_
#  - _foreach_log1p
#  - _foreach_log1p_
#  - _foreach_log2
#  - _foreach_log2_
#  - _foreach_maximum.List
#  - _foreach_maximum.Scalar
#  - _foreach_maximum.ScalarList
#  - _foreach_maximum_.List
#  - _foreach_maximum_.Scalar
#  - _foreach_maximum_.ScalarList
#  - _foreach_minimum.List
#  - _foreach_minimum.Scalar
#  - _foreach_minimum.ScalarList
#  - _foreach_minimum_.List
#  - _foreach_minimum_.Scalar
#  - _foreach_minimum_.ScalarList
#  - _foreach_mul.Tensor
#  - _foreach_mul_.Tensor
#  - _foreach_neg
#  - _foreach_neg_
#  - _foreach_pow.List
#  - _foreach_pow.Scalar
#  - _foreach_pow.ScalarAndTensor
#  - _foreach_pow.ScalarList
#  - _foreach_pow_.List
#  - _foreach_pow_.Scalar
#  - _foreach_pow_.ScalarList
#  - _foreach_reciprocal
#  - _foreach_reciprocal_
#  - _foreach_round
#  - _foreach_round_
#  - _foreach_sigmoid
#  - _foreach_sigmoid_
#  - _foreach_sign
#  - _foreach_sign_
#  - _foreach_sin
#  - _foreach_sin_
#  - _foreach_sinh
#  - _foreach_sinh_
#  - _foreach_sub.List
#  - _foreach_sub.Scalar
#  - _foreach_sub.ScalarList
#  - _foreach_sub_.List
#  - _foreach_sub_.Scalar
#  - _foreach_sub_.ScalarList
#  - _foreach_tan
#  - _foreach_tan_
#  - _foreach_tanh
#  - _foreach_tanh_
#  - _foreach_trunc
#  - _foreach_trunc_
#  - _foreach_zero_
#  - _fused_adam_
#  - _fused_adam_.tensor_lr
#  - _fused_adamw_
#  - _fused_adamw_.tensor_lr
#  - _fused_dropout
#  - _fused_moving_avg_obs_fq_helper
#  - _fused_sdp_choice
#  - _logcumsumexp
#  - _logcumsumexp.out
#  - _masked_scale
#  - _masked_softmax
#  - _masked_softmax_backward
#  - _native_multi_head_attention
#  - _pdist_backward
#  - _pdist_forward
#  - _sample_dirichlet
#  - _segment_reduce_backward
#  - _standard_gamma
#  - _standard_gamma_grad
#  - _transform_bias_rescale_qkv
#  - _upsample_nearest_exact3d.out
#  - _upsample_nearest_exact3d_backward.grad_input
#  - adaptive_avg_pool3d.out
#  - adaptive_avg_pool3d_backward.grad_input
#  - adaptive_max_pool3d.out
#  - adaptive_max_pool3d_backward.grad_input
#  - avg_pool3d.out
#  - avg_pool3d_backward.grad_input
#  - batch_norm_gather_stats
#  - batch_norm_gather_stats_with_counts
#  - binomial
#  - cauchy_
#  - channel_shuffle
#  - dequantize.self
#  - embedding_renorm_
#  - fake_quantize_per_channel_affine_cachemask
#  - fake_quantize_per_tensor_affine_cachemask
#  - fractional_max_pool2d.output
#  - fractional_max_pool2d_backward.grad_input
#  - fractional_max_pool3d.output
#  - fractional_max_pool3d_backward
#  - fractional_max_pool3d_backward.grad_input
#  - frexp.Tensor_out
#  - geometric_
#  - glu_backward_jvp
#  - glu_jvp
#  - grid_sampler_3d
#  - grid_sampler_3d_backward
#  - hardshrink
#  - hardshrink.out
#  - hardshrink_backward
#  - hardshrink_backward.grad_input
#  - heaviside.out
#  - histc
#  - histc.out
#  - i0.out
#  - igamma.out
#  - igammac.out
#  - index_copy.out
#  - isneginf.out
#  - isposinf.out
#  - kthvalue.values
#  - lcm
#  - lcm.out
#  - lcm_
#  - linspace.out
#  - log_normal_
#  - logspace.out
#  - max.dim
#  - max_pool3d_with_indices
#  - max_pool3d_with_indices.out
#  - max_pool3d_with_indices_backward
#  - max_pool3d_with_indices_backward.grad_input
#  - max_unpool2d
#  - max_unpool2d.out
#  - max_unpool3d
#  - max_unpool3d.out
#  - mean
#  - min.dim
#  - mode
#  - multi_margin_loss
#  - multi_margin_loss.out
#  - multi_margin_loss_backward
#  - multi_margin_loss_backward.grad_input
#  - multilabel_margin_loss_backward
#  - multilabel_margin_loss_backward.grad_input
#  - multilabel_margin_loss_forward
#  - multilabel_margin_loss_forward.output
#  - mv
#  - mvlgamma.out
#  - poisson
#  - put_
#  - record_stream
#  - rrelu_with_noise
#  - rrelu_with_noise.out
#  - rrelu_with_noise_
#  - rrelu_with_noise_backward
#  - segment_reduce
#  - sinc.out
#  - smooth_l1_loss_backward
#  - special_airy_ai.out
#  - special_bessel_j0.out
#  - special_bessel_j1.out
#  - special_bessel_y0.out
#  - special_bessel_y1.out
#  - special_chebyshev_polynomial_t.out
#  - special_chebyshev_polynomial_u.out
#  - special_chebyshev_polynomial_v.out
#  - special_chebyshev_polynomial_w.out
#  - special_entr.out
#  - special_erfcx.out
#  - special_hermite_polynomial_h.out
#  - special_hermite_polynomial_he.out
#  - special_i0e.out
#  - special_i1.out
#  - special_i1e.out
#  - special_laguerre_polynomial_l.out
#  - special_legendre_polynomial_p.out
#  - special_log_ndtr.out
#  - special_modified_bessel_i0.out
#  - special_modified_bessel_i1.out
#  - special_modified_bessel_k0.out
#  - special_modified_bessel_k1.out
#  - special_ndtri.out
#  - special_scaled_modified_bessel_k0.out
#  - special_scaled_modified_bessel_k1.out
#  - special_shifted_chebyshev_polynomial_t.out
#  - special_shifted_chebyshev_polynomial_u.out
#  - special_shifted_chebyshev_polynomial_v.out
#  - special_shifted_chebyshev_polynomial_w.out
#  - special_spherical_bessel_j0.out
#  - special_xlog1py.out
#  - special_zeta.out
#  - sum
#  - take
#  - take.out
#  - to_sparse
#  - tril_indices
#  - triu_indices
#  - trunc.out
#  - upsample_bicubic2d_backward.grad_input
#  - upsample_nearest3d.out
#  - upsample_nearest3d_backward.grad_input
#  - upsample_trilinear3d.out
#  - upsample_trilinear3d_backward.grad_input
#  - xlogy.OutTensor
  - _int_mm
autograd:
#  - cdist
  - lstm.input
  - gru.input
  - _scaled_dot_product_attention_math
