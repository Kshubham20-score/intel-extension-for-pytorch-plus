backend: XPU
cpp_namespace: at
class_name: AtenIpexTypeXPU
use_out_as_primary: true
device_guard: true
supported:
####################################
# Here list these ops conflicted with torch-xpu-ops fallback. We have to mute
# them here and register them with IPEX_TORCH_LIBRARY_IMPL to suppress the
# previously registered kernel overriding warning message.
#
# - angle
# - angle.out
################## override below ops due to UTs failed
# - ceil.out                           # for https://github.com/intel/torch-xpu-ops/pull/872
# - floor.out
# - col2im                             # for https://github.com/intel/torch-xpu-ops/pull/894
# - col2im.out
# - im2col
# - im2col.out
# - _embedding_bag
# - _embedding_bag_forward_only
# - glu_backward
# - glu_backward.grad_input
# - index_select
# - index_select.out
  - sort
  - sort.stable
  - sort.values
  - sort.values_stable
################## override below ops due to performance issues
# - convolution_overrideable
# - convolution_backward_overrideable
# - _addmm_activation.out
# - addmm.out
# - addmv.out
# - mm
# - mm.out
# - baddbmm
# - baddbmm.out
# - baddbmm_
# - addbmm
# - addbmm_
# - addbmm.out
# - bmm
# - bmm.out
# - tensordot.out
# - native_batch_norm
# - native_batch_norm.out
# - native_batch_norm_backward
# - upsample_bilinear2d
# - upsample_bilinear2d.out
# - upsample_bilinear2d_backward
# - upsample_bilinear2d_backward.grad_input
# - max_pool2d_with_indices
# - max_pool2d_with_indices.out
# - max_pool2d_with_indices_backward
# - max_pool2d_with_indices_backward.grad_input
# - _index_put_impl_
# - nonzero
# - nonzero.out
# - sum.IntList_out
# - sum.dim_IntList
# - nansum
# - nansum.out
#  - native_group_norm
#  - native_group_norm_backward
####################################
  - _assert_async
  - _assert_async.msg
  - _cdist_backward
  - _compute_linear_combination
  - _compute_linear_combination.out
  - _convert_indices_from_coo_to_csr.out
  - _convert_indices_from_csr_to_coo.out
  - _ctc_loss
  - _ctc_loss.Tensor
  - _ctc_loss_backward
  - _ctc_loss_backward.Tensor
  - _cummax_helper
  - _cummin_helper
  - _dirichlet_grad
  - _embedding_bag_dense_backward
  - _embedding_bag_per_sample_weights_backward
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
  - _fake_quantize_learnable_per_channel_affine
  - _fake_quantize_learnable_per_tensor_affine
  - _fake_quantize_learnable_per_channel_affine_backward
  - _fake_quantize_learnable_per_tensor_affine_backward
  - _fake_quantize_per_tensor_affine_cachemask_tensor_qparams
#  - _fft_c2r
#  - _fft_r2c
#  - _fft_c2c
#  - _fft_c2r.out
#  - _fft_r2c.out
#  - _fft_c2c.out
  - _fused_adam_
  - _fused_adam_.tensor_lr
  - _fused_adamw_
  - _fused_adamw_.tensor_lr
  - _fused_dropout
  - _fused_moving_avg_obs_fq_helper
  - _masked_scale
  - index_copy.out
#  - linalg_qr.out
#  - _local_scalar_dense
  - _logcumsumexp
  - _logcumsumexp.out
#  - _lu_with_info
  - _make_per_channel_quantized_tensor
  - _make_per_tensor_quantized_tensor
  - _masked_softmax
  - _masked_softmax_backward
  - _pdist_backward
  - _pdist_forward
  - _segment_reduce_backward
  - _sample_dirichlet
#  - _linalg_svd.U
#  - linalg_inv_ex.inverse
  - _linalg_solve_ex.result
#  - linalg_lu_factor_ex.out
#  - linalg_lu_solve.out
#  - linalg_lu.out
#  - linalg_matrix_exp
  - _adaptive_avg_pool3d
  - adaptive_avg_pool3d.out
  - _adaptive_avg_pool3d_backward
  - adaptive_avg_pool3d_backward.grad_input
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d_backward.grad_input
#  - add.Scalar
#  - add.Tensor
#  - add.out
#  - add_.Tensor
#  - amax.out
#  - avg_pool2d.out
  - avg_pool3d.out
  - avg_pool3d_backward.grad_input
  - batch_norm_gather_stats
  - batch_norm_gather_stats_with_counts
  - batch_norm_stats.out
#  - bitwise_not #Redundant registration
#  - bitwise_not_ #Redundant registration
#  - bitwise_left_shift.Tensor_Scalar #Redundant registration
#  - bitwise_left_shift_.Tensor_Scalar #Redundant registration
#  - bitwise_left_shift.Tensor_Scalar_out #Redundant registration
#  - bitwise_left_shift.Scalar_Tensor #Redundant registration
#  - bitwise_right_shift.Tensor_Scalar #Redundant registration
#  - bitwise_right_shift_.Tensor_Scalar #Redundant registration
#  - bitwise_right_shift.Tensor_Scalar_out #Redundant registration
#  - bitwise_right_shift.Scalar_Tensor #Redundant registration
  - cauchy_
#  - cholesky
#  - cholesky.out
#  - cholesky_inverse
#  - cholesky_inverse.out
#  - _cholesky_solve_helper
#  - linalg_cholesky_ex.L
  - conv_tbc
#  - copy_
  - cross
  - cross.out
  - dequantize.self
  - diag
  - diag.out
#  - dot
#  - silu.out
#  - empty.memory_format
#  - empty_strided
#  - equal
  - embedding_renorm_
  - fake_quantize_per_channel_affine_cachemask
  - fake_quantize_per_tensor_affine_cachemask
  - fractional_max_pool2d.output
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool3d.output
  - fractional_max_pool3d_backward
  - fractional_max_pool3d_backward.grad_input
  - frexp.Tensor_out
#  - gelu.out
  - geometric_
#  - geqrf
#  - geqrf.a
  - glu_backward_jvp
  - glu_jvp
  - grid_sampler_3d
  - grid_sampler_3d_backward
  - hardshrink
  - hardshrink.out
  - hardshrink_backward
  - hardshrink_backward.grad_input
  - heaviside.out
  - histc
  - histc.out
  - i0.out
  - igamma.out
  - igammac.out
  - _unsafe_index.Tensor
#  - inverse
#  - inverse.out
  - isneginf.out
  - isposinf.out
  - kthvalue.values
  - l1_loss
  - linspace.out
  - log_normal_
  - logspace.out
#  - lu_solve
#  - lu_solve.out
#  - lu_unpack
#  - lu_unpack.out
  - max.dim
  - max_pool3d_with_indices
  - max_pool3d_with_indices.out
  - max_pool3d_with_indices_backward
  - max_pool3d_with_indices_backward.grad_input
  - max_unpool2d
  - max_unpool2d.out
  - max_unpool3d
  - max_unpool3d.out
  - mean
#  - mean.out
#  - min
  - min.dim
  - mode
#  - mul.Tensor
#  - mul.Scalar
#  - mul.out
  - multi_margin_loss
  - multi_margin_loss.out
  - multi_margin_loss_backward
  - multi_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_forward
  - multilabel_margin_loss_forward.output
  - mv
  - mvlgamma.out
#  - ne.Scalar
#  - ne.Tensor
#  - ormqr
#  - ormqr.out
#  - pow.Tensor_Scalar_out
  - put_
  - quantize_per_channel
  - quantize_per_tensor
  - quantize_per_tensor.tensor_qparams
  - quantize_per_tensor_dynamic
  - record_stream
#  - relu
#  - relu_
  - resize_as_
  - rot90
  - rrelu_with_noise
  - rrelu_with_noise.out
  - rrelu_with_noise_
  - rrelu_with_noise_backward
  - segment_reduce
  - sinc.out
  - smooth_l1_loss_backward
#  - smooth_l1_loss_backward.grad_input
  - soft_margin_loss
  - soft_margin_loss.out
  - soft_margin_loss_backward
  - soft_margin_loss_backward.grad_input
#  - softplus
#  - softplus.out
  - special_entr.out
  - special_erfcx.out
  - special_i0e.out
  - special_i1.out
  - special_i1e.out
  - special_ndtri.out
  - special_xlog1py.out
  - special_zeta.out
  - special_bessel_j0.out
  - special_bessel_j1.out
  - special_bessel_y0.out
  - special_bessel_y1.out
  - special_chebyshev_polynomial_t.out
  - special_chebyshev_polynomial_u.out
  - special_chebyshev_polynomial_v.out
  - special_chebyshev_polynomial_w.out
  - special_modified_bessel_i0.out
  - special_modified_bessel_i1.out
  - special_modified_bessel_k0.out
  - special_modified_bessel_k1.out
  - special_scaled_modified_bessel_k0.out
  - special_scaled_modified_bessel_k1.out
  - special_log_ndtr.out
  - special_hermite_polynomial_h.out
  - special_hermite_polynomial_he.out
  - special_laguerre_polynomial_l.out
  - special_legendre_polynomial_p.out
  - special_spherical_bessel_j0.out
  - special_shifted_chebyshev_polynomial_t.out
  - special_shifted_chebyshev_polynomial_u.out
  - special_shifted_chebyshev_polynomial_v.out
  - special_shifted_chebyshev_polynomial_w.out
  - special_airy_ai.out
  - sum
  - take
  - take.out
  - to_sparse
  - to_sparse.sparse_dim
  - _transformer_encoder_layer_fwd
#  - triangular_solve
#  - triangular_solve.X
  - tril_indices
  - triu_indices
  - trunc.out
  - upsample_bicubic2d_backward.grad_input
  - upsample_nearest3d.out
  - upsample_nearest3d_backward.grad_input
  - upsample_trilinear3d.out
  - upsample_trilinear3d_backward.grad_input
  - _upsample_nearest_exact3d.out
  - _upsample_nearest_exact3d_backward.grad_input
  - xlogy.OutTensor
  - channel_shuffle
  - _linalg_slogdet.sign
  - _linalg_det.result
#  - linalg_eig
#  - linalg_eig.out
#  - linalg_eigh
#  - linalg_eigh.eigvals
#  - linalg_eigvalsh
#  - linalg_eigvalsh.out
#  - _linalg_eigh.eigenvalues
#  - _linalg_eigvals
  - linalg_eigvals.out
#  - linalg_householder_product
#  - linalg_householder_product.out
#  - linalg_solve_triangular
#  - linalg_solve_triangular.out
#  - vdot
  - square.out
  - _foreach_abs
  - _foreach_abs_
  - _foreach_copy_
  - _foreach_sigmoid
  - _foreach_sigmoid_
  - _foreach_round
  - _foreach_round_
  - _foreach_frac
  - _foreach_frac_
  - _foreach_reciprocal
  - _foreach_reciprocal_
  - _foreach_erfc
  - _foreach_erfc_
  - _foreach_expm1
  - _foreach_expm1_
  - _foreach_lgamma
  - _foreach_lgamma_
  - _foreach_pow.Scalar
  - _foreach_pow_.Scalar
  - _foreach_pow.ScalarAndTensor
  - _foreach_pow.List
  - _foreach_pow_.List
  - _foreach_pow.ScalarList
  - _foreach_pow_.ScalarList
  - _foreach_trunc
  - _foreach_trunc_
  - _foreach_floor
  - _foreach_floor_
  - _foreach_ceil
  - _foreach_ceil_
  - _foreach_acos
  - _foreach_acos_
  - _foreach_asin
  - _foreach_asin_
  - _foreach_atan
  - _foreach_atan_
  - _foreach_cosh
  - _foreach_cosh_
  - _foreach_tan
  - _foreach_tan_
  - _foreach_sin
  - _foreach_sin_
  - _foreach_sinh
  - _foreach_sinh_
  - _foreach_sign
  - _foreach_sign_
  - _foreach_exp
  - _foreach_exp_
  - _foreach_tanh
  - _foreach_tanh_
  - _foreach_log
  - _foreach_log_
  - _foreach_log10
  - _foreach_log10_
  - _foreach_log2
  - _foreach_log2_
  - _foreach_cos
  - _foreach_cos_
  - _foreach_log1p
  - _foreach_log1p_
  - _foreach_erf
  - _foreach_erf_
  - _foreach_neg
  - _foreach_neg_
  - _foreach_zero_
  - _foreach_maximum.List
  - _foreach_maximum_.List
  - _foreach_minimum.List
  - _foreach_minimum_.List
  - _thnn_fused_gru_cell
  - _thnn_fused_gru_cell_backward
  - _standard_gamma_grad
  - lcm
  - lcm.out
  - lcm_
  - _convert_weight_to_int4pack
  - _foreach_sub.Scalar
  - _foreach_sub_.Scalar
  - _foreach_maximum.Scalar
  - _foreach_maximum_.Scalar
  - _foreach_minimum.Scalar
  - _foreach_minimum_.Scalar
  - _foreach_sub.List
  - _foreach_sub_.List
  - _foreach_sub.ScalarList
  - _foreach_sub_.ScalarList
  - _foreach_maximum.ScalarList
  - _foreach_maximum_.ScalarList
  - _foreach_minimum.ScalarList
  - _foreach_minimum_.ScalarList
  - _foreach_clamp_min.Scalar
  - _foreach_clamp_min_.Scalar
  - _foreach_clamp_max.Scalar
  - _foreach_clamp_max_.Scalar
  - _foreach_clamp_max.List
  - _foreach_clamp_max.ScalarList
  - _foreach_clamp_max_.List
  - _foreach_clamp_max_.ScalarList
  - _foreach_clamp_min.List
  - _foreach_clamp_min.ScalarList
  - _foreach_clamp_min_.List
  - _foreach_clamp_min_.ScalarList
  - _foreach_mul.Tensor
  - _foreach_mul_.Tensor
#  - matrix_exp
  - binomial
  - _standard_gamma
  - poisson
  - _weight_int4pack_mm
  - _validate_compressed_sparse_indices
  - pad_sequence
  - _fused_sdp_choice
  - _scaled_dot_product_efficient_attention
  - _scaled_dot_product_efficient_attention_backward
  - _transform_bias_rescale_qkv
  - _native_multi_head_attention
  - _nested_from_padded
autograd:
  - cdist
  - lstm.input
  - gru.input
  - _scaled_dot_product_attention_math
